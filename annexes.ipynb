{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des matières\n",
    "\n",
    "* [Codes supplémentaires](#section1)\n",
    "    * [Multi-Threading](#section11)\n",
    "    * [Travail sur le poids des fichiers](#section12)\n",
    "        * [Zip](#section121)\n",
    "        * [Partition des fichiers](#section122)\n",
    "* [Nettoyage des données de Légifrance](#section2)\n",
    "    * [xxx](#section21)\n",
    "* [Sauvegarde des tableaux de données finalisées](#section3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes supplémentaires<a class=\"anchor\" id=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section seront regroupés tous les codes qui ont été faits, en partie ou totalement, mais qui ne sont finalement pas intégrés au projet final. Nous les avons mis car ils sont potentiellement intéressants, car explorant d'autre méthodes par exemple, et sont en lien avec le projet, donc pourraient servir à l'avenir dans des projets similaires. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Threading <a class=\"anchor\" id=\"section11\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de nos tentatives de récupération des données via l'API Piste, une méthode envisagée fût de récupérer les fichiers sur des pages de de taille 1, car cela semblait fonctionner au delà du 10001e éléments. Ainsi pour pallier les contraintes de temps du projet nous avons codé de quoi faire simultanément les récupération de données en multi-threading. \n",
    "\n",
    "L'idée est donc de calculer le nombre de pages restantes pour la requête souhaitée, puis en donnant le nombre de threads souhaités produire le même nombre de fonctions. Ces fonctions récupéreront les données sur des plages de pages de tailles similaires, les plages correspondent environ au ratio de pages restantes par rapport aux nombres de fonctions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remaining_page_number(file_name):\n",
    "    '''\n",
    "    Calcule le nombre de pages restantes à récupérer en comparant le nombre total de résultats et la progression sauvegardée dans un fichier spécifique.\n",
    "\n",
    "    :param file_name: Nom du fichier JSON utilisé pour sauvegarder la progression (sans l'extension).\n",
    "    :return: Le nombre de pages restantes à récupérer à partir de la progression sauvegardée.\n",
    "    '''\n",
    "    api_host = API_HOST+\"/search\"\n",
    "    client = get_client()\n",
    "    response = client.post(api_host, json=code_api).json()\n",
    "    total_results = response.get(\"totalResultNumber\", 0)\n",
    "    file_name = str(file_name)+\".json\"\n",
    "    try:\n",
    "        # Charger les données existantes si le fichier existe\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            existing_data = json.load(file)\n",
    "            if not isinstance(existing_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            start_page = existing_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        start_page = 0\n",
    "    remaining_page = total_results-start_page\n",
    "    return remaining_page    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_results_between(api_host, code, page_to_start, page_to_end, thread_number):\n",
    "    '''\n",
    "    Récupère les résultats d'une requête API sur une plage spécifique de pages.\n",
    "\n",
    "    :param api_host: Adresse du serveur où envoyer la requête avec le endpoint correspondant.\n",
    "    :param code: Code de la requête en json\n",
    "    :param page_to_start: Numéro de la première page à récupérer dans la plage.\n",
    "    :param page_to_end: Numéro de la dernière page à récupérer dans la plage.\n",
    "    :param thread_number: Numéro du thread utilisé pour différencier les fichiers de sauvegarde.\n",
    "    :return: Aucun retour direct, les résultats sont sauvegardés dans un fichier JSON.\n",
    "    '''\n",
    "\n",
    "    client = get_client()\n",
    "    expires_in = 55*60\n",
    "    token_expiry = datetime.now() + timedelta(seconds=expires_in)\n",
    "\n",
    "    file_name = str(thread_number)+\"results.json\"\n",
    "\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for page_number in range(page_to_start, page_to_end + 1):\n",
    "        # Vérifier si le token doit être renouvelé\n",
    "        if datetime.now() >= token_expiry:\n",
    "            print(\"Renouvellement du client OAuth...\")\n",
    "            client = get_client()\n",
    "            token_expiry = datetime.now() + timedelta(seconds=expires_in)\n",
    "\n",
    "        print(f\"Récupération de la page {page_number}/{page_to_end - page_to_start +1}...\")\n",
    "        code[\"recherche\"][\"pageNumber\"] = page_number\n",
    "        response = client.post(api_host, json=code).json()\n",
    "        page_results = response.get(\"results\", [])\n",
    "\n",
    "        if response.get(\"error\") == 503:\n",
    "            print(response)\n",
    "            break\n",
    "\n",
    "        if page_number % 10 == 0: \n",
    "            print(response)\n",
    "\n",
    "        # Ajouter les résultats de la page courante\n",
    "        all_results.extend(page_results)\n",
    "\n",
    "        # Sauvegarder les résultats toutes les 20 pages ou à la dernière page\n",
    "        if page_number % 20 == 0 or page_number == page_to_end:\n",
    "            print(f\"Ajout des pages jusqu'à la page {page_number} dans {file_name}...\")\n",
    "            save_results_to_file(all_results, file_name, page_number)\n",
    "\n",
    "            # Réinitialiser la liste des résultats sauvegardés\n",
    "            all_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_functions(n, file_name):\n",
    "        '''\n",
    "        Génère un ensemble de fonctions pour récupérer des pages de résultats en parallèle.\n",
    "\n",
    "        :param n: Nombre de threads ou de fonctions à générer.\n",
    "        :param file_name: Nom du fichier JSON utilisé pour sauvegarder la progression (sans l'extension).\n",
    "        :return: Un dictionnaire contenant les fonctions générées, prêtes à être exécutées pour traiter une plage de pages.\n",
    "        '''\n",
    "\n",
    "        api_host = API_HOST+\"/search\"\n",
    "        client = get_client()\n",
    "        remaining_page = remaining_page_number(file_name)\n",
    "        functions = {}\n",
    "        file_name = str(file_name)+\".json\"\n",
    "        response = client.post(API_HOST+\"/search\", json=code_api).json()\n",
    "        total_results = response.get(\"totalResultNumber\", 0)\n",
    "        try:\n",
    "            with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "                existing_data = json.load(file)\n",
    "                if not isinstance(existing_data, dict):\n",
    "                    raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "                start_page = existing_data.get(\"current_page\", 1)\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            start_page = 1\n",
    "\n",
    "        page_state = [start_page, start_page]\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            if i != n : \n",
    "                    page_to_end = [int(np.floor(remaining_page*i/n)+ page_state[1])]\n",
    "            else : \n",
    "                    page_to_end = [total_results]\n",
    "\n",
    "            thread_number = i\n",
    "\n",
    "            def func_template(idx=i, start=page_state[0], end=page_to_end[0], thread_nbr= thread_number):\n",
    "                collect_all_results_between(API_HOST+\"/search\", code_api, start, end, thread_nbr)\n",
    "\n",
    "            page_state[0]= page_to_end[0] + 1\n",
    "                \n",
    "            functions[f\"f_{i}\"] = func_template\n",
    "        return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functions_to_thread(n, file_name):\n",
    "    '''\n",
    "    Prépare une liste de fonctions générées avec leurs arguments pour une exécution dans des threads.\n",
    "\n",
    "    :param n: Nombre de threads ou de fonctions à générer.\n",
    "    :param file_name: Nom du fichier JSON utilisé pour sauvegarder la progression (sans l'extension).\n",
    "    :return: Une liste de tuples contenant les fonctions générées, leurs arguments (liste vide), et leurs mots-clés (dictionnaire vide).\n",
    "    '''\n",
    "\n",
    "    generated_functions = generate_functions(n, file_name)\n",
    "    functions = [(generated_functions[f\"f_{i}\"], [], {}) for i in range(1, n+1) ]\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_threads(functions):\n",
    "    '''\n",
    "    Exécute un ensemble de fonctions dans des threads distincts.\n",
    "\n",
    "    :param functions: Liste de tuples contenant les fonctions à exécuter, leurs arguments (sous forme de liste), et leurs mots-clés (sous forme de dictionnaire).\n",
    "    :return: Aucun retour direct. Les threads sont lancés et exécutés, puis attendus jusqu'à leur complétion.\n",
    "    '''\n",
    "\n",
    "    threads = []\n",
    "\n",
    "    for func, args, kwargs in functions:\n",
    "        thread = Thread(target=func, args=args, kwargs=kwargs)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # Attendre la fin de tous les threads\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail sur le poids des fichiers <a class=\"anchor\" id=\"section12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de la récolte des données de Légifrance nous avons fait face à des fichiers très lourds, parfois proche du giga, dès lors deux choses ont été entreprises plus ou moins partiellement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip <a class=\"anchor\" id=\"section121\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première méthode et la plus classique consistait en une compression des fichiers au format Zip, ce qui se révéla très peu efficace car les fichiers json sont déjà bien compressés par essence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipper_fichier(fichier, zip_nom):\n",
    "    \"\"\"\n",
    "    Crée un fichier ZIP contenant le fichier spécifié et supprime le fichier d'origine\n",
    "\n",
    "    :param fichier: Chemin du fichier à zipper.\n",
    "    :param zip_nom: Nom du fichier ZIP de sortie.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_nom, 'w') as zipf:\n",
    "        zipf.write(fichier, arcname=fichier.split('/')[-1]) \n",
    "        os.remove(fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_json_du_zip(fichier_zip, fichier_sortie):\n",
    "    \"\"\"\n",
    "    Extrait un fichier JSON contenu dans une archive ZIP et le sauvegarde.\n",
    "\n",
    "    :param fichier_zip: Chemin de l'archive ZIP contenant le fichier JSON.\n",
    "    :param fichier_sortie: Chemin du fichier JSON extrait.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(fichier_zip, 'r') as zipf:\n",
    "        json_fichier = zipf.namelist()[0]  \n",
    "        with zipf.open(json_fichier) as file:\n",
    "            data = json.load(file)\n",
    "        with open(fichier_sortie, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Partition des fichiers <a class=\"anchor\" id=\"section122\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seconde méthode est restée au stade de brouillon mais consistait en une partition des fichier json afin de pouvoir les envoyer segmentés sur github avant de les reconstruire au moment de l'exécution des fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmenter_json_par_parties(fichier_json, dossier_sortie, nombre_parties):\n",
    "    \"\"\"\n",
    "    Segmente un fichier JSON en un nombre spécifique de parties.\n",
    "\n",
    "    :param fichier_json: Chemin du fichier JSON à segmenter.\n",
    "    :param dossier_sortie: Dossier où les segments seront sauvegardés.\n",
    "    :param nombre_parties: Nombre de parties dans lesquelles le fichier sera segmenté.\n",
    "    \"\"\"\n",
    "    # Lire le contenu du fichier JSON\n",
    "    with open(fichier_json, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Calculer la taille approximative de chaque partie\n",
    "    taille_segment = math.ceil(len(data) / nombre_parties)\n",
    "\n",
    "    # Créer le dossier de sortie s'il n'existe pas\n",
    "    os.makedirs(dossier_sortie, exist_ok=True)\n",
    "\n",
    "    # Segmenter les données\n",
    "    for i in range(0, len(data), taille_segment):\n",
    "        segment = data[i:i + taille_segment]\n",
    "        segment_path = os.path.join(dossier_sortie, f'segment_{i // taille_segment + 1}.json')\n",
    "        \n",
    "        # Écrire chaque segment dans un fichier\n",
    "        with open(segment_path, 'w', encoding='utf-8') as segment_file:\n",
    "            json.dump(segment, segment_file, indent=4)\n",
    "    \n",
    "    print(f\"Fichier JSON segmenté en {nombre_parties} parties dans le dossier '{dossier_sortie}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembler_json(dossier_segments, fichier_sortie):\n",
    "    \"\"\"\n",
    "    Assemble plusieurs fichiers JSON en un seul fichier et supprime les segments.\n",
    "\n",
    "    :param dossier_segments: Dossier contenant les segments JSON.\n",
    "    :param fichier_sortie: Chemin du fichier JSON de sortie.\n",
    "    \"\"\"\n",
    "    fichiers = sorted(os.listdir(dossier_segments))  # Trier les segments par nom\n",
    "    data_combinee = []\n",
    "\n",
    "    for fichier in fichiers:\n",
    "        segment_path = os.path.join(dossier_segments, fichier)\n",
    "        with open(segment_path, 'r', encoding='utf-8') as segment_file:\n",
    "            data_combinee.extend(json.load(segment_file))\n",
    "\n",
    "    # Écrire les données combinées dans un seul fichier JSON\n",
    "    with open(fichier_sortie, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(data_combinee, output_file, indent=4)\n",
    "    \n",
    "    print(f\"Segments JSON assemblés dans '{fichier_sortie}', et les segments supprimés.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
