{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on récupère les données via PISTE l'API permettant d'avoir accès aux données de Légifrance, le code de connexion OAuth est donné sur le Gitlab de Piste à cette [adresse](https://gitlab.com/piste_lab/oauth_connectors/-/blob/master/Python/Oauth2ClientCredentialsTest.py?ref_type=heads). Il est tout de même nécessaire d'installer les modules du code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.12/site-packages (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from requests-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from requests-oauthlib) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.0.0->requests-oauthlib) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.0.0->requests-oauthlib) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.0.0->requests-oauthlib) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.0.0->requests-oauthlib) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install requests-oauthlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from requests_oauthlib import OAuth2Session\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Thread\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client():\n",
    "    \"\"\"\n",
    "    Récupère un client OAuth2Session configuré avec un token d'accès depuis le serveur OAuth.\n",
    "\n",
    "    :return: Un objet OAuth2Session prêt à être utilisé pour des requêtes API.\n",
    "    \"\"\"\n",
    "    TOKEN_URL = \"https://oauth.piste.gouv.fr/api/oauth/token\"\n",
    "\n",
    "    # Charger les identifiants client depuis le fichier .env\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv(\"CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "    # Requête pour obtenir le token\n",
    "    res = requests.post(\n",
    "        TOKEN_URL,\n",
    "        data={\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": client_id,\n",
    "            \"client_secret\": client_secret,\n",
    "            \"scope\": \"openid\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    res.raise_for_status()  # Lever une erreur si la requête échoue\n",
    "    token = res.json()\n",
    "\n",
    "    # Retourner un client OAuth2Session configuré\n",
    "    return OAuth2Session(client_id, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(results, file_name, current_page):\n",
    "    \"\"\"\n",
    "    Sauvegarde les résultats dans un fichier JSON. Si le fichier existe, ajoute les nouvelles données.\n",
    "\n",
    "    :param results: Liste des résultats à sauvegarder.\n",
    "    :param file_name: Nom du fichier JSON.\n",
    "    :param current_page: La page actuelle traitée.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les données existantes si le fichier existe\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            existing_data = json.load(file)\n",
    "            if not isinstance(existing_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            existing_results = existing_data.get(\"results\", [])\n",
    "            start_page = existing_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        existing_results = []\n",
    "        start_page = 1\n",
    "\n",
    "    # Ajouter les nouveaux résultats\n",
    "    existing_results.extend(results)\n",
    "\n",
    "    # Sauvegarder les résultats mis à jour avec la page actuelle\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"results\": existing_results, \"current_page\": current_page}, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_results(api_host, code):\n",
    "    # Nom unique pour le fichier de sauvegarde\n",
    "    file_name = \"results.json\"\n",
    "\n",
    "    # Charger la page de démarrage depuis le fichier de sauvegarde\n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            saved_data = json.load(file)\n",
    "            if not isinstance(saved_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            start_page = saved_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        print(f\"Fichier {file_name} introuvable ou mal structuré. Démarrage depuis la première page.\")\n",
    "        start_page = 1\n",
    "\n",
    "    # Initialiser le client OAuth2Session et gérer le renouvellement\n",
    "    client = get_client()\n",
    "    expires_in = 55*60\n",
    "    token_expiry = datetime.now() + timedelta(seconds=expires_in)  # Renouveler 1 minute avant expiration\n",
    "\n",
    "    # Récupérer le total de résultats et calculer le nombre de pages\n",
    "    response = client.post(api_host, json=code).json()\n",
    "    total_results = response.get(\"totalResultNumber\", 0)\n",
    "    page_size = code[\"recherche\"][\"pageSize\"]\n",
    "    total_pages = math.ceil(total_results / page_size)\n",
    "\n",
    "    print(f\"Total de résultats : {total_results}\")\n",
    "    print(f\"Nombre de pages à récupérer : {total_pages}\")\n",
    "    print(f\"Reprise à partir de la page {start_page}\")\n",
    "\n",
    "    # Liste pour stocker les résultats courants\n",
    "    all_results = []\n",
    "\n",
    "    for page_number in range(start_page, total_pages + 1):\n",
    "        # Vérifier si le token doit être renouvelé\n",
    "        if datetime.now() >= token_expiry:\n",
    "            print(\"Renouvellement du client OAuth...\")\n",
    "            client = get_client()\n",
    "            token_expiry = datetime.now() + timedelta(seconds=expires_in)\n",
    "\n",
    "        print(f\"Récupération de la page {page_number}/{total_pages}...\")\n",
    "        code[\"recherche\"][\"pageNumber\"] = page_number\n",
    "        response = client.post(api_host, json=code).json()\n",
    "        page_results = response.get(\"results\", [])\n",
    "\n",
    "        if response.get(\"error\") == 503:\n",
    "            print(response)\n",
    "            break\n",
    "\n",
    "        if page_number % 10 == 0: \n",
    "            print(response)\n",
    "\n",
    "        # Ajouter les résultats de la page courante\n",
    "        all_results.extend(page_results)\n",
    "\n",
    "        # Sauvegarder les résultats toutes les 20 pages ou à la dernière page\n",
    "        if page_number % 20 == 0 or page_number == total_pages:\n",
    "            print(f\"Ajout des pages jusqu'à la page {page_number} dans {file_name}...\")\n",
    "            save_results_to_file(all_results, file_name, page_number)\n",
    "\n",
    "            # Réinitialiser la liste des résultats sauvegardés\n",
    "            all_results = []\n",
    "\n",
    "    print(f\"Récupération terminée. Dernière page sauvegardée : {total_pages}\")\n",
    "    return total_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_HOST = \"https://api.piste.gouv.fr/dila/legifrance/lf-engine-app\"\n",
    "\n",
    "code_api_LODA = {\n",
    "  \"recherche\": {\n",
    "    \"filtres\": [\n",
    "      {\n",
    "        \"dates\": {\n",
    "          \"start\": \"1996-01-01\",\n",
    "          \"end\": \"2022-08-31\"\n",
    "        },\n",
    "        \"facette\": \"DATE_SIGNATURE\"\n",
    "      }\n",
    "    ],\n",
    "    \"sort\": \"SIGNATURE_DATE_DESC\",\n",
    "    \"fromAdvancedRecherche\": True,\n",
    "    \"champs\": [\n",
    "      {\n",
    "        \"criteres\": [\n",
    "          {\n",
    "            \"valeur\": \"délinquance\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"crime\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"délit\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Homicides\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Vols\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Stupéfiants\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Escroquerie\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Contrefaçon\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Sequestrations\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Recels\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Proxénétisme\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Menaces\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Cambriolages\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"infraction\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Attentats\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"dégradations\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          },\n",
    "          {\n",
    "            \"valeur\": \"Outrages\",\n",
    "            \"operateur\": \"OU\",\n",
    "            \"typeRecherche\": \"UN_DES_MOTS\"\n",
    "          }\n",
    "        ],\n",
    "        \"operateur\": \"OU\",\n",
    "        \"typeChamp\": \"ALL\"\n",
    "      }\n",
    "    ],\n",
    "    \"pageSize\": 1,\n",
    "    \"pageNumber\": 100,\n",
    "    \"operateur\": \"ET\",\n",
    "    \"typePagination\": \"DEFAUT\"\n",
    "  },\n",
    "  \"fond\":\"LODA_DATE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_HOST = \"https://api.piste.gouv.fr/dila/legifrance/lf-engine-app\"\n",
    "\n",
    "code_api = {\n",
    "    \"recherche\": {\n",
    "        \"champs\": [\n",
    "            {\n",
    "                \"typeChamp\": \"ALL\",\n",
    "                \"criteres\": [\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"délinquance\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"crime\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"délit\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Homicides\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Vols\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Stupéfiants\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Escroquerie\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Contrefaçon\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Sequestrations\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Recels\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Proxénétisme\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Menaces\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Cambriolages\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"infraction\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Attentats\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"dégradations\", \"operateur\": \"OU\"},\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": \"Outrages\", \"operateur\": \"OU\"}\n",
    "                ],\n",
    "                \"operateur\": \"OU\"\n",
    "            }\n",
    "        ],\n",
    "        \"pageNumber\": 1,\n",
    "        \"pageSize\": 1,\n",
    "        \"operateur\": \"ET\",\n",
    "        \"sort\": \"DATE\",\n",
    "        \"typePagination\": \"DEFAUT\"\n",
    "    },\n",
    "    \"fond\": \"ALL\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collect_all_results(API_HOST+\"/search\", code_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipper_fichier(fichier, zip_nom):\n",
    "    \"\"\"\n",
    "    Crée un fichier ZIP contenant le fichier spécifié et supprime le fichier d'origine\n",
    "\n",
    "    :param fichier: Chemin du fichier à zipper.\n",
    "    :param zip_nom: Nom du fichier ZIP de sortie.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_nom, 'w') as zipf:\n",
    "        zipf.write(fichier, arcname=fichier.split('/')[-1]) \n",
    "        os.remove(fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_json_du_zip(fichier_zip, fichier_sortie):\n",
    "    \"\"\"\n",
    "    Extrait un fichier JSON contenu dans une archive ZIP et le sauvegarde.\n",
    "\n",
    "    :param fichier_zip: Chemin de l'archive ZIP contenant le fichier JSON.\n",
    "    :param fichier_sortie: Chemin du fichier JSON extrait.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(fichier_zip, 'r') as zipf:\n",
    "        json_fichier = zipf.namelist()[0]  \n",
    "        with zipf.open(json_fichier) as file:\n",
    "            data = json.load(file)\n",
    "        with open(fichier_sortie, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipper_fichier('/home/onyxia/work/Python-pour-la-data-science-2A/data/results.json', 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraire_json_du_zip('/home/onyxia/work/Python-pour-la-data-science-2A/data/results.zip', '/home/onyxia/work/Python-pour-la-data-science-2A/data/results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/onyxia/work/Python-pour-la-data-science-2A/data'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_threads(functions):\n",
    "    \"\"\"\n",
    "    Lance un ensemble de fonctions dans des threads séparés.\n",
    "\n",
    "    :param functions: Liste de tuples (fonction, args, kwargs) où :\n",
    "                      - `fonction` est la fonction à exécuter\n",
    "                      - `args` est une liste des arguments positionnels\n",
    "                      - `kwargs` est un dictionnaire des arguments nommés\n",
    "    \"\"\"\n",
    "    threads = []\n",
    "\n",
    "    for func, args, kwargs in functions:\n",
    "        thread = Thread(target=func, args=args, kwargs=kwargs)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # Attendre la fin de tous les threads\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remaining_page_number():\n",
    "    response = client.post(api_host, json=code).json()\n",
    "    total_results = response.get(\"totalResultNumber\", 0)\n",
    "    try:\n",
    "        # Charger les données existantes si le fichier existe\n",
    "        with open(results.json(), \"r\", encoding=\"utf-8\") as file:\n",
    "            existing_data = json.load(file)\n",
    "            if not isinstance(existing_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            start_page = existing_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        existing_results = []\n",
    "        start_page = 1\n",
    "    remaining_page = total_results-start_page\n",
    "    return remaining_page    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_functions(n):\n",
    "        remaining_page = remaining_page_number()\n",
    "        functions = {}\n",
    "        for i in range(1, n + 1):\n",
    "            def func_template(idx=i):\n",
    "                \n",
    "            functions[f\"f_{i}\"] = func_template\n",
    "        return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmenter_json_par_parties(fichier_json, dossier_sortie, nombre_parties):\n",
    "    \"\"\"\n",
    "    Segmente un fichier JSON en un nombre spécifique de parties.\n",
    "\n",
    "    :param fichier_json: Chemin du fichier JSON à segmenter.\n",
    "    :param dossier_sortie: Dossier où les segments seront sauvegardés.\n",
    "    :param nombre_parties: Nombre de parties dans lesquelles le fichier sera segmenté.\n",
    "    \"\"\"\n",
    "    # Lire le contenu du fichier JSON\n",
    "    with open(fichier_json, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Calculer la taille approximative de chaque partie\n",
    "    taille_segment = math.ceil(len(data) / nombre_parties)\n",
    "\n",
    "    # Créer le dossier de sortie s'il n'existe pas\n",
    "    os.makedirs(dossier_sortie, exist_ok=True)\n",
    "\n",
    "    # Segmenter les données\n",
    "    for i in range(0, len(data), taille_segment):\n",
    "        segment = data[i:i + taille_segment]\n",
    "        segment_path = os.path.join(dossier_sortie, f'segment_{i // taille_segment + 1}.json')\n",
    "        \n",
    "        # Écrire chaque segment dans un fichier\n",
    "        with open(segment_path, 'w', encoding='utf-8') as segment_file:\n",
    "            json.dump(segment, segment_file, indent=4)\n",
    "    \n",
    "    print(f\"Fichier JSON segmenté en {nombre_parties} parties dans le dossier '{dossier_sortie}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "slice(0, 1, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msegmenter_json_par_parties\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/onyxia/work/Python-pour-la-data-science-2A/data/results.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m, in \u001b[0;36msegmenter_json_par_parties\u001b[0;34m(fichier_json, dossier_sortie, nombre_parties)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Segmenter les données\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data), taille_segment):\n\u001b[0;32m---> 21\u001b[0m     segment \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtaille_segment\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m     segment_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dossier_sortie, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegment_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtaille_segment\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Écrire chaque segment dans un fichier\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: slice(0, 1, None)"
     ]
    }
   ],
   "source": [
    "segmenter_json_par_parties(\"/home/onyxia/work/Python-pour-la-data-science-2A/data/results.json\", \"data\", 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembler_json(dossier_segments, fichier_sortie):\n",
    "    \"\"\"\n",
    "    Assemble plusieurs fichiers JSON en un seul fichier et supprime les segments.\n",
    "\n",
    "    :param dossier_segments: Dossier contenant les segments JSON.\n",
    "    :param fichier_sortie: Chemin du fichier JSON de sortie.\n",
    "    \"\"\"\n",
    "    fichiers = sorted(os.listdir(dossier_segments))  # Trier les segments par nom\n",
    "    data_combinee = []\n",
    "\n",
    "    for fichier in fichiers:\n",
    "        segment_path = os.path.join(dossier_segments, fichier)\n",
    "        with open(segment_path, 'r', encoding='utf-8') as segment_file:\n",
    "            data_combinee.extend(json.load(segment_file))\n",
    "\n",
    "    # Écrire les données combinées dans un seul fichier JSON\n",
    "    with open(fichier_sortie, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(data_combinee, output_file, indent=4)\n",
    "    \n",
    "    print(f\"Segments JSON assemblés dans '{fichier_sortie}', et les segments supprimés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pour envoyer des requêtes à l'API nous devons d'abord obtenir un token, après s'être inscrit sur le [site de PISTE](https://piste.gouv.fr/). Ce token est obtenu en envoyant une requête contenant mon identifiant client et mon code de client au site d'autorisation. Nous avons ensuite une autorisation d'une heure avec ce token pour exploiter l'API de Légifrance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant en utilisant le lien d'exploitation de l'API auquel on rajoute l'endpoints qui permet d'accéder à ce que l'on veut faire nous récupérons les données. La liste des endpoints pour l'API de Légifrance est disponible [ici](https://piste.gouv.fr/index.php?option=com_apiportal&view=apitester&usage=api&apitab=tests&apiName=L%C3%A9gifrance&apiId=7e5a0e1d-ffcc-40be-a405-a1a5c1afe950&managerId=3&type=rest&apiVersion=2.4.2&Itemid=179&swaggerVersion=2.0&lang=fr)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous permet de récupérer un ensemble de documents appartenant à l'ensemble des lois, ordonnance, décrets et arrêtés de Légifrance entre le premier janvier 1996 et le 31 août 2022 qui contiennent au moins un des mots d'une liste définie. Cette liste comprend les termes des taux de délinquances détaillés [ici](database_délinquance.ipynb#Calcul-des-taux-de-délinquance). Ce code est un json et sera envoyé par une méthode post à l'API, elle nous renverra un nombre de résultats limités (100 ici). Ce code est très inspiré de celui disponible sur le [site de l'API](https://piste.gouv.fr/index.php?option=com_apiportal&view=apitester&usage=api&apitab=tests&apiName=L%C3%A9gifrance&apiId=7e5a0e1d-ffcc-40be-a405-a1a5c1afe950&managerId=3&type=rest&apiVersion=2.4.2&Itemid=179&swaggerVersion=2.0&lang=fr) à l'endpoint /search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit une fonction qui nous permet de récupérer tous les documents de notre recherche, car chaque recherche est limitée à une page de 100 éléments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def results_to_dataframe(json_data):\n",
    "    data = []\n",
    "    \n",
    "    # Accéder à la liste des résultats\n",
    "    results = json_data.get(\"results\", [])\n",
    "    \n",
    "    for result in results:\n",
    "        \n",
    "        # Extraire les informations requises\n",
    "        titles = result.get(\"titles\")\n",
    "        title_info = titles[0] if isinstance(titles, list) and titles else {}\n",
    "        \n",
    "        title = title_info.get(\"title\")\n",
    "        id = title_info.get(\"id\")\n",
    "        date = result.get(\"date\")\n",
    "        nature = result.get(\"nature\")\n",
    "        etat = result.get(\"etat\")\n",
    "        origin = result.get(\"origin\")\n",
    "        date_publication = result.get(\"datePublication\")\n",
    "        \n",
    "        # Ajouter les données dans la liste\n",
    "        data.append({\n",
    "            \"Titre\": title,\n",
    "            \"ID\": id,\n",
    "            \"Date\": date,\n",
    "            \"Nature\": nature,\n",
    "            \"Etat\": etat,\n",
    "            \"Origine\": origin,\n",
    "            \"Date Publication\": date_publication\n",
    "        })\n",
    "    \n",
    "    # Créer et retourner un DataFrame\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def results_to_dataframe(json_data):\n",
    "    data = []\n",
    "    \n",
    "    # Accéder à la liste des résultats\n",
    "    results = json_data.get(\"results\", [])\n",
    "    \n",
    "    if not isinstance(results, list):\n",
    "        raise ValueError(\"La clé 'results' doit contenir une liste.\")\n",
    "    \n",
    "    for result in results:\n",
    "        if not isinstance(result, dict):\n",
    "            continue  # Ignorer les éléments non dictionnaires\n",
    "        \n",
    "        # Extraire les informations requises\n",
    "        titles = result.get(\"titles\")\n",
    "        title_info = titles[0] if isinstance(titles, list) and titles else {}\n",
    "        \n",
    "        if not isinstance(title_info, dict):\n",
    "            title_info = {}\n",
    "        \n",
    "        title = title_info.get(\"title\")\n",
    "        id = title_info.get(\"id\")\n",
    "        date = result.get(\"date\")\n",
    "        nature = result.get(\"nature\")\n",
    "        etat = result.get(\"etat\")\n",
    "        origin = result.get(\"origin\")\n",
    "        date_publication = result.get(\"datePublication\")\n",
    "        \n",
    "        # Ajouter les données dans la liste\n",
    "        data.append({\n",
    "            \"Titre\": title,\n",
    "            \"ID\": id,\n",
    "            \"Date\": date,\n",
    "            \"Nature\": nature,\n",
    "            \"Etat\": etat,\n",
    "            \"Origine\": origin,\n",
    "            \"Date Publication\": date_publication\n",
    "        })\n",
    "    \n",
    "    # Créer et retourner un DataFrame\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# results_to_dataframe(votre_json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_LODA.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_data = json.load(file)\n",
    "df_LODA = results_to_dataframe(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LODA = df_LODA.drop_duplicates()\n",
    "df_LODA.to_csv(\"resultats_legifrance_loda.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
