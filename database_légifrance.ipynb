{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de données répertoriant les nouveaux textes législatifs (depuis 2006) en France"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table des matières\n",
    "\n",
    "* [Récupération des données de Légifrance via une API](#section1)\n",
    "    * [Installation et importation des modules](#section11)\n",
    "    * [Requêtes sur l'API](#section12)\n",
    "    * [Travail sur les fichiers extraits](#section13)\n",
    "* [Nettoyage des données de Légifrance](#section2)\n",
    "    * [xxx](#section21)\n",
    "* [Sauvegarde des tableaux de données finalisées](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook contient les différents codes qui nous ont permis d'accéder aux données disponibles grâce à l'API Piste de Légifrance.\n",
    "\n",
    "Sur l'API de Légifrance les requêtes doivent être faites sur un fonds, soit un filtre correspondant à une catégorie spécifique de la base de données de Légifrance. Les différents fonds incluent notamment LODA , qui regroupe les lois, les ordonnances, les décrets et les arrêtés, mais il y a aussi CODE pour les documents relatifs aux différents codes et ALL qui permet de faire une requête sur tous les fonds. \n",
    "\n",
    "Au début de notre récupération des données nos requêtes ont été effectuées sur le fond LODA, ensuite CODE et enfin ALL, avant de revenir sur LODA. Cet ordre n'est dû qu'à l'absence de documentation claire, au faible nombre de projets présents sur internet utilisant l'API de Légifrance et d'erreurs que l'on n'arrive pas à expliquer et qui nous empêchent d'avoir accès à l'intégralité des fonds. \n",
    "\n",
    "En fait, lorsqu'on effectue une requête, l'API ne nous renvoie qu'un nombre limité de résultats (100 maximum), donc il faut faire une boucle pour récupérer tous les documents. Les requêtes supérieures au 10001e éléments renvoient une erreur 503, soit une erreur du serveur. Malgré des recherches et des mails envoyés au support et à des personnes travaillant à l'AIFE (Agence pour l'Informatique Financière de l'État) sur l'API de Légifrance nous n'avons pas trouvé de solution à cela. \n",
    "\n",
    "Ainsi, pour l'avancement du projet, nous avons pris la décision de ne récupérer que les données sur le fond LODA, car ce fond est d'après nous le plus pertinent - les modifications des codes étant plus difficiles à étudier sous l'angle quantitatif par rapport aux lois, car il s'agit en partie de modification de vocabulaire employé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données avec le fond LODA <a class=\"anchor\" id=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation et importation des modules <a class=\"anchor\" id=\"section11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install requests-oauthlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from requests_oauthlib import OAuth2Session\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Thread\n",
    "import zipfile\n",
    "import numpy as np \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requêtes sur l'API <a class=\"anchor\" id=\"section12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pour envoyer des requêtes à l'API nous devons d'abord obtenir un token, après s'être inscrit sur le [site de PISTE](https://piste.gouv.fr/). Ce token est obtenu en envoyant une requête contenant mon identifiant client et mon code de client au site d'autorisation, ces derniers seront écrits dans un fichier .env à chaque fois qu'on lance le code pour récupérer un client. Nous avons ensuite une autorisation d'une heure avec ce token pour exploiter l'API de Légifrance. \n",
    "\n",
    "Le code pour récupérer le token est grandement inspiré de celui proposé par le Gitlab de Piste présent à cette [adresse](https://gitlab.com/piste_lab/oauth_connectors/-/blob/master/Python/Oauth2ClientCredentialsTest.py?ref_type=heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client():\n",
    "    \"\"\"\n",
    "    Récupère un client OAuth2Session configuré avec un token d'accès depuis le serveur OAuth. \n",
    "\n",
    "    :return: Un objet OAuth2Session prêt à être utilisé pour des requêtes API.\n",
    "    \"\"\"\n",
    "    TOKEN_URL = \"https://oauth.piste.gouv.fr/api/oauth/token\"\n",
    "\n",
    "    # Charger les identifiants client depuis le fichier .env\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv(\"CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "    # Requête pour obtenir le token\n",
    "    res = requests.post(\n",
    "        TOKEN_URL,\n",
    "        data={\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": client_id,\n",
    "            \"client_secret\": client_secret,\n",
    "            \"scope\": \"openid\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    res.raise_for_status()  # Lever une erreur si la requête échoue\n",
    "\n",
    "    token = res.json()\n",
    "\n",
    "    # Retourner un client OAuth2Session configuré\n",
    "    return OAuth2Session(client_id, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant en utilisant le lien d'exploitation de l'API auquel on rajoute l'endpoints qui permet d'accéder à ce que l'on veut faire nous récupérons les données. La liste des endpoints pour l'API de Légifrance est disponible [ici](https://piste.gouv.fr/index.php?option=com_apiportal&view=apitester&usage=api&apitab=tests&apiName=L%C3%A9gifrance&apiId=7e5a0e1d-ffcc-40be-a405-a1a5c1afe950&managerId=3&type=rest&apiVersion=2.4.2&Itemid=179&swaggerVersion=2.0&lang=fr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_HOST = \"https://api.piste.gouv.fr/dila/legifrance/lf-engine-app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les codes ci-dessous sont les requêtes que l'on va envoyer à l'API, ces dernières sont en json et pour en envoyer une il faut taper client.post(api_url, json=code).json(), selon l'endpoint il faudra mettre get à la place de post. \n",
    "\n",
    "Les codes ci-dessous permettent de récupérer un ensemble de documents appartenant à l'ensemble des lois, ordonnance, décrets et arrêtés de Légifrance entre le premier janvier 1996 et le 31 août 2022 qui contiennent au moins un des mots d'une liste définie. Cette liste comprend les termes des taux de délinquances détaillés [ici](database_délinquance.ipynb#Calcul-des-taux-de-délinquance). Ce code est un json et sera envoyé par une méthode post à l'API, elle nous renverra un nombre de résultats limités (100 ici). Ce code est très inspiré de celui disponible sur le [site de l'API](https://piste.gouv.fr/index.php?option=com_apiportal&view=apitester&usage=api&apitab=tests&apiName=L%C3%A9gifrance&apiId=7e5a0e1d-ffcc-40be-a405-a1a5c1afe950&managerId=3&type=rest&apiVersion=2.4.2&Itemid=179&swaggerVersion=2.0&lang=fr) à l'endpoint /search.\n",
    "\n",
    "La seule différence entre les deux codes résident dans le filtre temporel, car comme l'API ne permet pas de renvoyer plus de 10001 résultats par requête alors nous sommes obligés d'en faire deux distinctes et de regrouper les données a posteriori. Cela aurait pu fonctionner sur le fond All en fractionnant encore plus, or nous n'avons pas réussi à faire fonctionner le filtre temporel dans une requête sur All. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_api_LODA_1 = {\n",
    "    \"recherche\": {\n",
    "        \"filtres\": [{\"dates\": {\"start\": \"2008-01-01\", \"end\": \"2022-08-31\"}, \"facette\": \"DATE_SIGNATURE\"}],\n",
    "        \"sort\": \"SIGNATURE_DATE_DESC\",\n",
    "        \"fromAdvancedRecherche\": True,\n",
    "        \"champs\": [\n",
    "            {\n",
    "                \"typeChamp\": \"ALL\",\n",
    "                \"criteres\": [\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": mot, \"operateur\": \"OU\"} for mot in [\n",
    "                        \"délinquance\", \"crime\", \"délit\", \"Homicides\", \"Vols\", \"Stupéfiants\", \"Escroquerie\",\n",
    "                        \"Contrefaçon\", \"Sequestrations\", \"Recels\", \"Proxénétisme\", \"Menaces\", \"Cambriolages\",\n",
    "                        \"infraction\", \"Attentats\", \"dégradations\", \"Outrages\"\n",
    "                    ]\n",
    "                ],\n",
    "                \"operateur\": \"OU\"\n",
    "            }\n",
    "        ],\n",
    "        \"pageSize\": 100,\n",
    "        \"pageNumber\": 1,\n",
    "        \"operateur\": \"ET\",\n",
    "        \"typePagination\": \"DEFAUT\"\n",
    "    },\n",
    "    \"fond\": \"LODA_DATE\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_api_LODA_2 = {\n",
    "    \"recherche\": {\n",
    "        \"filtres\": [{\"dates\": {\"start\": \"1996-01-01\", \"end\": \"1999-12-31\"}, \"facette\": \"DATE_SIGNATURE\"}],\n",
    "        \"sort\": \"SIGNATURE_DATE_DESC\",\n",
    "        \"fromAdvancedRecherche\": True,\n",
    "        \"champs\": [\n",
    "            {\n",
    "                \"typeChamp\": \"ALL\",\n",
    "                \"criteres\": [\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": mot, \"operateur\": \"OU\"} for mot in [\n",
    "                        \"délinquance\", \"crime\", \"délit\", \"Homicides\", \"Vols\", \"Stupéfiants\", \"Escroquerie\",\n",
    "                        \"Contrefaçon\", \"Sequestrations\", \"Recels\", \"Proxénétisme\", \"Menaces\", \"Cambriolages\",\n",
    "                        \"infraction\", \"Attentats\", \"dégradations\", \"Outrages\"\n",
    "                    ]\n",
    "                ],\n",
    "                \"operateur\": \"OU\"\n",
    "            }\n",
    "        ],\n",
    "        \"pageSize\": 100,\n",
    "        \"pageNumber\": 1,\n",
    "        \"operateur\": \"ET\",\n",
    "        \"typePagination\": \"DEFAUT\"\n",
    "    },\n",
    "    \"fond\": \"LODA_DATE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la limite de résultats est de 100 pour une requête nous avons créer deux fonctions complémentaire, l'une parcourant toutes les pages de notre requêtes afin de récupérer tous les résultats tout en s'appuyant sur la seconde qui permet de les sauvegarder dans un fichier json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(results, file_name, current_page):\n",
    "    \"\"\"\n",
    "    Sauvegarde les résultats dans un fichier JSON. Si le fichier existe, ajoute les nouvelles données.\n",
    "\n",
    "    :param results: Liste des résultats à sauvegarder.\n",
    "    :param file_name: Nom du fichier JSON.\n",
    "    :param current_page: La page actuelle traitée.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les données existantes si le fichier existe\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            existing_data = json.load(file)\n",
    "            if not isinstance(existing_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            existing_results = existing_data.get(\"results\", [])\n",
    "            start_page = existing_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        existing_results = []\n",
    "        start_page = 1\n",
    "\n",
    "    # Ajouter les nouveaux résultats\n",
    "    existing_results.extend(results)\n",
    "\n",
    "    # Sauvegarder les résultats mis à jour avec la page actuelle\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"results\": existing_results, \"current_page\": current_page}, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_results(api_host, code, file_name):\n",
    "    ''' \n",
    "    Récupère tous les résultats relatifs à une requête API. \n",
    "\n",
    "    :param api_host: addresse à laquelle envoyer la requête avec le endpoint correspondant \n",
    "    :param code: code permettant de faire la requête correspondante à nos recherches, \n",
    "    pour l'utilisation de la fonction on mettra de fait \n",
    "    collect_all_results(api_host, json = code)\n",
    "    \n",
    "    :return: le nombre total de page \n",
    "    '''\n",
    "\n",
    "    # Charger la page de démarrage depuis le fichier de sauvegarde s'il existe comme il faut \n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            saved_data = json.load(file)\n",
    "            if not isinstance(saved_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            start_page = saved_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        print(f\"Fichier {file_name} introuvable ou mal structuré. Démarrage depuis la première page.\")\n",
    "        start_page = 1\n",
    "\n",
    "    # Initialiser le client OAuth2Session \n",
    "    client = get_client()\n",
    "\n",
    "    # On fixe un temps d'expiration à 55 min car le token dure 60 min\n",
    "    expires_in = 55*60\n",
    "    token_expiry = datetime.now() + timedelta(seconds=expires_in) \n",
    "\n",
    "    # Récupérer le total de résultats et calculer le nombre de pages\n",
    "    response = client.post(api_host, json=code).json()\n",
    "    total_results = response.get(\"totalResultNumber\", 0)\n",
    "    page_size = code[\"recherche\"][\"pageSize\"]\n",
    "    total_pages = math.ceil(total_results / page_size)\n",
    "\n",
    "    print(f\"Total de résultats : {total_results}\")\n",
    "    print(f\"Nombre de pages à récupérer : {total_pages}\")\n",
    "    print(f\"Reprise à partir de la page {start_page}\")\n",
    "\n",
    "    # Liste pour stocker les résultats courants\n",
    "    all_results = []\n",
    "\n",
    "    # On boucle sur le nombre de pages\n",
    "    for page_number in range(start_page, total_pages + 1):\n",
    "\n",
    "        # Vérifier si le token doit être renouvelé\n",
    "        if datetime.now() >= token_expiry:\n",
    "            print(\"Renouvellement du client OAuth...\")\n",
    "            client = get_client()\n",
    "            token_expiry = datetime.now() + timedelta(seconds=expires_in)\n",
    "        \n",
    "        # Récupère le bon numéro de page et lance la requête\n",
    "        print(f\"Récupération de la page {page_number}/{total_pages}...\")\n",
    "        code[\"recherche\"][\"pageNumber\"] = page_number\n",
    "        response = client.post(api_host, json=code).json()\n",
    "        page_results = response.get(\"results\", [])\n",
    "\n",
    "        # Teste si une erreur 503 arrive et arrête la boucle si tel est le cas\n",
    "        if response.get(\"error\") == 503:\n",
    "            print(response)\n",
    "            break\n",
    "\n",
    "        # Affiche une requête toutes les 10 pour du contrôle \n",
    "        if page_number % 10 == 0: \n",
    "            print(response)\n",
    "\n",
    "        # Ajouter les résultats de la page courante\n",
    "        all_results.extend(page_results)\n",
    "\n",
    "        # Sauvegarder les résultats toutes les 20 pages ou à la dernière page\n",
    "        if page_number % 20 == 0 or page_number == total_pages:\n",
    "            print(f\"Ajout des pages jusqu'à la page {page_number} dans {file_name}...\")\n",
    "            save_results_to_file(all_results, file_name, page_number)\n",
    "\n",
    "            # Réinitialiser la liste des résultats sauvegardés\n",
    "            all_results = []\n",
    "\n",
    "    print(f\"Récupération terminée. Dernière page sauvegardée : {total_pages}\")\n",
    "    return total_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1996_to_2008 = collect_all_results(API_HOST+\"/search\", code_api_LODA_2, \"results_1996_to_2008.json\")\n",
    "results_2008_to_2022 = collect_all_results(API_HOST+\"/search\", code_api_LODA_1, \"results_2008_to_2022.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Travail sur les fichiers extraits <a class=\"anchor\" id=\"section13\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir récupéré nos deux fichiers json il faut les convertir en dataframe puis en csv afin de les exploiter par la suite. Nous avons ainsi codé une fonction permettant d'extraire la date du titre des documents, une autre qui les convertit en dataframe en ne gardant qu'un nombre limité d'informations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_title(title):\n",
    "    \"\"\"\n",
    "    Extrait la date d'un titre. Si plusieurs dates sont présentes, retourne la plus récente.\n",
    "\n",
    "    :param title: Le titre de l'objet.\n",
    "    :return: La date extraite ou None si aucune date valide n'est trouvée.\n",
    "    \"\"\"\n",
    "    # Regex pour les formats de date\n",
    "    date_patterns = [\n",
    "        r\"(\\d{1,2})\\s+(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)\\s+(\\d{4})\",\n",
    "        r\"(\\d{1,2}/\\d{1,2}/\\d{4})\"\n",
    "    ]\n",
    "\n",
    "    found_dates = []\n",
    "\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, title, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            if len(match) == 3:  # Format \"11 mai 2005\"\n",
    "                day, month, year = match\n",
    "                month_mapping = {\n",
    "                    \"janvier\": 1, \"février\": 2, \"mars\": 3, \"avril\": 4, \"mai\": 5, \"juin\": 6,\n",
    "                    \"juillet\": 7, \"août\": 8, \"septembre\": 9, \"octobre\": 10, \"novembre\": 11, \"décembre\": 12\n",
    "                }\n",
    "                month_num = month_mapping[month.lower()]\n",
    "                found_dates.append(datetime(int(year), month_num, int(day)))\n",
    "            elif len(match) == 1:  # Format \"12/12/2014\"\n",
    "                date_str = match[0]\n",
    "                found_dates.append(datetime.strptime(date_str, \"%d/%m/%Y\"))\n",
    "\n",
    "    if found_dates:\n",
    "        return max(found_dates).strftime(\"%Y-%m-%d\")  # Retourne la date la plus récente au format AAAA-MM-JJ\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_dataframe(json_data):\n",
    "    \"\"\"\n",
    "    Convertit les données JSON en DataFrame en extrayant des champs spécifiques.\n",
    "\n",
    "    :param json_data: Les données JSON à analyser.\n",
    "    :return: Un DataFrame contenant les données extraites.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Accéder à la liste des résultats\n",
    "    results = json_data.get(\"results\", [])\n",
    "\n",
    "    for result in results:\n",
    "        # Extraire les informations requises\n",
    "        titles = result.get(\"titles\")\n",
    "        title_info = titles[0] if isinstance(titles, list) and titles else {}\n",
    "\n",
    "        title = title_info.get(\"title\")\n",
    "        id = title_info.get(\"id\")\n",
    "        date = extract_date_from_title(title)  # Extraire uniquement à partir du titre\n",
    "        nature = result.get(\"nature\")\n",
    "        etat = result.get(\"etat\")\n",
    "        origin = result.get(\"origin\")\n",
    "        date_publication = result.get(\"datePublication\")\n",
    "\n",
    "        # Ajouter les données dans la liste\n",
    "        data.append({\n",
    "            \"Titre\": title,\n",
    "            \"ID\": id,\n",
    "            \"Date\": date,\n",
    "            \"Nature\": nature,\n",
    "            \"Etat\": etat,\n",
    "            \"Origine\": origin,\n",
    "            \"Date Publication\": date_publication\n",
    "        })\n",
    "\n",
    "    # Créer et retourner un DataFrame\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite nous appliquons ces fonctions à nos deux fichiers json avant de les concaténer sous un unique et de le transformer en csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_1996_to_2008.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "results_1996_to_2008 = results_to_dataframe(json_data)\n",
    "\n",
    "with open(\"results_2008_to_2022.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "results_2008_to_2022 = results_to_dataframe(json_data)\n",
    "\n",
    "results_1996_to_2022 = pd.concat([results_1996_to_2008, results_2008_to_2022], ignore_index=True)\n",
    "\n",
    "results_1996_to_2022.to_csv(\"results_LODA.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là c'est la suite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le fichier CSV\n",
    "chemin_fichier = \"data/resultats_legifrance_loda.csv\"\n",
    "df_loda = pd.read_csv(chemin_fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Nature</th>\n",
       "      <th>Etat</th>\n",
       "      <th>Origine</th>\n",
       "      <th>Date Publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3065</th>\n",
       "      <td>Arrêté du 22 mars 2016 portant approbation du ...</td>\n",
       "      <td>LEGITEXT000038810959_26-03-2016</td>\n",
       "      <td>2016-03-26T00:00:00.000+0000</td>\n",
       "      <td>ARRETE</td>\n",
       "      <td>VIGUEUR</td>\n",
       "      <td>LEGI</td>\n",
       "      <td>2016-03-25T00:00:00.000+0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Titre  \\\n",
       "3065  Arrêté du 22 mars 2016 portant approbation du ...   \n",
       "\n",
       "                                   ID                          Date  Nature  \\\n",
       "3065  LEGITEXT000038810959_26-03-2016  2016-03-26T00:00:00.000+0000  ARRETE   \n",
       "\n",
       "         Etat Origine              Date Publication  \n",
       "3065  VIGUEUR    LEGI  2016-03-25T00:00:00.000+0000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loda.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Nature</th>\n",
       "      <th>Etat</th>\n",
       "      <th>Origine</th>\n",
       "      <th>Date Publication</th>\n",
       "      <th>Mois</th>\n",
       "      <th>Année</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arrêté du 31 août 2022 fixant les niveaux de p...</td>\n",
       "      <td>LEGITEXT000046244600_21-08-2023</td>\n",
       "      <td>2022-09-01 00:00:00+00:00</td>\n",
       "      <td>ARRETE</td>\n",
       "      <td>VIGUEUR</td>\n",
       "      <td>LEGI</td>\n",
       "      <td>2022-09-01T00:00:00.000+0000</td>\n",
       "      <td>09</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arrêté du 30 août 2022 portant création de la ...</td>\n",
       "      <td>LEGITEXT000046664861_28-08-2023</td>\n",
       "      <td>2022-12-02 00:00:00+00:00</td>\n",
       "      <td>ARRETE</td>\n",
       "      <td>VIGUEUR</td>\n",
       "      <td>LEGI</td>\n",
       "      <td>2022-12-01T00:00:00.000+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arrêté du 30 août 2022 portant création de la ...</td>\n",
       "      <td>LEGITEXT000046664861_02-12-2022</td>\n",
       "      <td>2022-12-02 00:00:00+00:00</td>\n",
       "      <td>ARRETE</td>\n",
       "      <td>VIGUEUR</td>\n",
       "      <td>LEGI</td>\n",
       "      <td>2022-12-01T00:00:00.000+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arrêté du 29 août 2022 portant création de la ...</td>\n",
       "      <td>LEGITEXT000046666903_02-12-2022</td>\n",
       "      <td>2022-12-02 00:00:00+00:00</td>\n",
       "      <td>ARRETE</td>\n",
       "      <td>VIGUEUR</td>\n",
       "      <td>LEGI</td>\n",
       "      <td>2022-12-01T00:00:00.000+0000</td>\n",
       "      <td>12</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arrêté du 22 août 2022 fixant la liste des pos...</td>\n",
       "      <td>LEGITEXT000046230032_21-11-2022</td>\n",
       "      <td>2022-09-01 00:00:00+00:00</td>\n",
       "      <td>ARRETE</td>\n",
       "      <td>VIGUEUR</td>\n",
       "      <td>LEGI</td>\n",
       "      <td>2022-08-31T00:00:00.000+0000</td>\n",
       "      <td>09</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Titre  \\\n",
       "0  Arrêté du 31 août 2022 fixant les niveaux de p...   \n",
       "1  Arrêté du 30 août 2022 portant création de la ...   \n",
       "2  Arrêté du 30 août 2022 portant création de la ...   \n",
       "3  Arrêté du 29 août 2022 portant création de la ...   \n",
       "4  Arrêté du 22 août 2022 fixant la liste des pos...   \n",
       "\n",
       "                                ID                      Date  Nature     Etat  \\\n",
       "0  LEGITEXT000046244600_21-08-2023 2022-09-01 00:00:00+00:00  ARRETE  VIGUEUR   \n",
       "1  LEGITEXT000046664861_28-08-2023 2022-12-02 00:00:00+00:00  ARRETE  VIGUEUR   \n",
       "2  LEGITEXT000046664861_02-12-2022 2022-12-02 00:00:00+00:00  ARRETE  VIGUEUR   \n",
       "3  LEGITEXT000046666903_02-12-2022 2022-12-02 00:00:00+00:00  ARRETE  VIGUEUR   \n",
       "4  LEGITEXT000046230032_21-11-2022 2022-09-01 00:00:00+00:00  ARRETE  VIGUEUR   \n",
       "\n",
       "  Origine              Date Publication Mois Année  \n",
       "0    LEGI  2022-09-01T00:00:00.000+0000   09  2022  \n",
       "1    LEGI  2022-12-01T00:00:00.000+0000   12  2022  \n",
       "2    LEGI  2022-12-01T00:00:00.000+0000   12  2022  \n",
       "3    LEGI  2022-12-01T00:00:00.000+0000   12  2022  \n",
       "4    LEGI  2022-08-31T00:00:00.000+0000   09  2022  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On met la date sous format date de pandas\n",
    "df_loda['Date'] = pd.to_datetime(df_loda['Date']) \n",
    "\n",
    "# On ajoute la colonne mois et l'année via la variable Date (On a arbitrairement écarté Date Publication car les écarts sont de quelques jours uniquement)\n",
    "df_loda['Mois'] = df_loda['Date'].dt.month.apply(lambda x: f'{x:02}')\n",
    "df_loda['Année'] = df_loda['Date'].dt.year.astype(str)\n",
    "\n",
    "df_loda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs uniques dans la colonne 'année' :\n",
      "['2022' '2023' '2024' '2021' '2020' '2019' '2222' '2018' '2017' '2016'\n",
      " '2015' '2014' '2013' '2012' '2011' '2010' '2009' '2008' '2007' '2006']\n",
      "\n",
      "Valeurs uniques dans la colonne 'mois' :\n",
      "['09' '12' '08' '10' '07' '01' '05' '06' '04' '03' '02' '11']\n",
      "\n",
      "Valeurs uniques et leur fréquence dans la colonne 'Nature' :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Nature\n",
       "ARRETE        6790\n",
       "DECRET        1826\n",
       "LOI           1207\n",
       "ORDONNANCE     174\n",
       "DECISION         3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vérification pertinence des données\n",
    "print(\"Valeurs uniques dans la colonne 'année' :\")\n",
    "print(df_loda['Année'].unique())\n",
    "\n",
    "print(\"\\nValeurs uniques dans la colonne 'mois' :\")\n",
    "print(df_loda['Mois'].unique())\n",
    "\n",
    "print(\"\\nValeurs uniques et leur fréquence dans la colonne 'Nature' :\")\n",
    "df_loda['Nature'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se permet d'exclure pour le reste de l'étude les nouvelles normes législatives adoptées de type \"décision\" puisqu'elles ne sont qu'au nombre de 3 sur la période étudiée (trop faible et donc sûrement pas significatif pour le reste de l'analyse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loda = df_loda[df_loda['Nature'] != 'DECISION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des tableaux de données finalisées <a class=\"anchor\" id=\"section3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "Access Denied.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/s3fs/core.py:729\u001b[0m, in \u001b[0;36mS3FileSystem._lsdir\u001b[0;34m(self, path, refresh, max_items, delimiter, prefix, versions)\u001b[0m\n\u001b[1;32m    728\u001b[0m files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterdir(\n\u001b[1;32m    730\u001b[0m     bucket,\n\u001b[1;32m    731\u001b[0m     max_items\u001b[38;5;241m=\u001b[39mmax_items,\n\u001b[1;32m    732\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[1;32m    733\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[1;32m    734\u001b[0m     versions\u001b[38;5;241m=\u001b[39mversions,\n\u001b[1;32m    735\u001b[0m ):\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/s3fs/core.py:779\u001b[0m, in \u001b[0;36mS3FileSystem._iterdir\u001b[0;34m(self, bucket, max_items, delimiter, prefix, versions)\u001b[0m\n\u001b[1;32m    772\u001b[0m it \u001b[38;5;241m=\u001b[39m pag\u001b[38;5;241m.\u001b[39mpaginate(\n\u001b[1;32m    773\u001b[0m     Bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[1;32m    774\u001b[0m     Prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreq_kw,\n\u001b[1;32m    778\u001b[0m )\n\u001b[0;32m--> 779\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m i\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommonPrefixes\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/aiobotocore/paginate.py:30\u001b[0m, in \u001b[0;36mAioPageIterator.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(current_kwargs)\n\u001b[1;32m     31\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_parsed_response(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/aiobotocore/client.py:412\u001b[0m, in \u001b[0;36mAioBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    411\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m fs \u001b[38;5;241m=\u001b[39m s3fs\u001b[38;5;241m.\u001b[39mS3FileSystem(client_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://minio.lab.sspcloud.fr\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      3\u001b[0m MY_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manhlinh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMY_BUCKET\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Sauvegarde dans S3\u001b[39;00m\n\u001b[1;32m      7\u001b[0m MY_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manhlinh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/s3fs/core.py:1014\u001b[0m, in \u001b[0;36mS3FileSystem._ls\u001b[0;34m(self, path, detail, refresh, versions)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lsbuckets(refresh)\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1014\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lsdir(path, refresh, versions\u001b[38;5;241m=\u001b[39mversions)\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m path:\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/s3fs/core.py:742\u001b[0m, in \u001b[0;36mS3FileSystem._lsdir\u001b[0;34m(self, path, refresh, max_items, delimiter, prefix, versions)\u001b[0m\n\u001b[1;32m    740\u001b[0m     files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dirs\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m translate_boto_error(e)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delimiter \u001b[38;5;129;01mand\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m versions:\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdircache[path] \u001b[38;5;241m=\u001b[39m files\n",
      "\u001b[0;31mPermissionError\u001b[0m: Access Denied."
     ]
    }
   ],
   "source": [
    "'''\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "\n",
    "MY_BUCKET = \"williamolivier\"\n",
    "fs.ls(MY_BUCKET)\n",
    "\n",
    "# Sauvegarde dans S3\n",
    "MY_BUCKET = \"williamolivier\"\n",
    "FILE_PATH_OUT_S3 = f\"{MY_BUCKET}/diffusion/df_loda.csv\"\n",
    "\n",
    "with fs.open(FILE_PATH_OUT_S3, \"w\") as file_out:\n",
    "    df_loda.to_csv(file_out)\n",
    "\n",
    "fs.ls(f\"{MY_BUCKET}/diffusion\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
