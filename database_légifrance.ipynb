{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de données répertoriant les textes législatifs mentionnant les types d'infractions nous intéressants entre 1996 et 2022 en France\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table des matières\n",
    "\n",
    "* [Récupération des données de Légifrance via une API](#section1)\n",
    "    * [Installation et importation des modules](#section11)\n",
    "    * [Requêtes sur l'API](#section12)\n",
    "    * [Travail sur les fichiers extraits](#section13)\n",
    "* [Nettoyage des données de Légifrance](#section2)\n",
    "* [Sauvegarde des tableaux de données finalisées](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook contient les différents codes qui nous ont permis d'accéder aux données disponibles grâce à l'API Piste de Légifrance.\n",
    "\n",
    "Sur l'API de Légifrance les requêtes doivent être faites sur un fonds, soit un filtre correspondant à une catégorie spécifique de la base de données de Légifrance. Les différents fonds incluent notamment LODA , qui regroupe les lois, les ordonnances, les décrets et les arrêtés, mais il y a aussi CODE pour les documents relatifs aux différents codes et ALL qui permet de faire une requête sur tous les fonds. \n",
    "\n",
    "Au début de notre récupération des données nos requêtes ont été effectuées sur le fond LODA, ensuite CODE et enfin ALL, avant de revenir sur LODA. Cet ordre n'est dû qu'à l'absence de documentation claire, au faible nombre de projets présents sur internet utilisant l'API de Légifrance et d'erreurs que l'on n'arrive pas à expliquer et qui nous empêchent d'avoir accès à l'intégralité des fonds. \n",
    "\n",
    "En fait, lorsqu'on effectue une requête, l'API ne nous renvoie qu'un nombre limité de résultats (100 maximum), donc il faut faire une boucle pour récupérer tous les documents. Les requêtes supérieures au 10001e éléments renvoient une erreur 503, soit une erreur du serveur. Malgré des recherches et des mails envoyés au support et à des personnes travaillant à l'AIFE (Agence pour l'Informatique Financière de l'État) sur l'API de Légifrance nous n'avons pas trouvé de solution à cela. \n",
    "\n",
    "Ainsi, pour l'avancement du projet, nous avons pris la décision de ne récupérer que les données sur le fond LODA, car ce fond est d'après nous le plus pertinent - les modifications des codes étant plus difficiles à étudier sous l'angle quantitatif par rapport aux lois, car il s'agit en partie de modification de vocabulaire employé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données avec le fond LODA <a class=\"anchor\" id=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation et importation des modules <a class=\"anchor\" id=\"section11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from requests_oauthlib import OAuth2Session\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Thread\n",
    "import zipfile\n",
    "import numpy as np \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requêtes sur l'API <a class=\"anchor\" id=\"section12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pour envoyer des requêtes à l'API nous devons d'abord obtenir un token, après s'être inscrit sur le [site de PISTE](https://piste.gouv.fr/). Ce token est obtenu en envoyant une requête contenant mon identifiant client et mon code de client au site d'autorisation, ces derniers seront écrits dans un fichier .env à chaque fois qu'on lance le code pour récupérer un client. Nous avons ensuite une autorisation d'une heure avec ce token pour exploiter l'API de Légifrance. \n",
    "\n",
    "Le code pour récupérer le token est grandement inspiré de celui proposé par le Gitlab de Piste présent à cette [adresse](https://gitlab.com/piste_lab/oauth_connectors/-/blob/master/Python/Oauth2ClientCredentialsTest.py?ref_type=heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client():\n",
    "    \"\"\"\n",
    "    Récupère un client OAuth2Session configuré avec un token d'accès depuis le serveur OAuth. \n",
    "\n",
    "    :return: Un objet OAuth2Session prêt à être utilisé pour des requêtes API.\n",
    "    \"\"\"\n",
    "    TOKEN_URL = \"https://oauth.piste.gouv.fr/api/oauth/token\"\n",
    "\n",
    "    # Charger les identifiants client depuis le fichier .env\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv(\"CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "    # Requête pour obtenir le token\n",
    "    res = requests.post(\n",
    "        TOKEN_URL,\n",
    "        data={\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": client_id,\n",
    "            \"client_secret\": client_secret,\n",
    "            \"scope\": \"openid\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    res.raise_for_status()  # Lever une erreur si la requête échoue\n",
    "\n",
    "    token = res.json()\n",
    "\n",
    "    # Retourner un client OAuth2Session configuré\n",
    "    return OAuth2Session(client_id, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant en utilisant le lien d'exploitation de l'API auquel on rajoute l'endpoints qui permet d'accéder à ce que l'on veut faire nous récupérons les données. La liste des endpoints pour l'API de Légifrance est disponible [ici](https://piste.gouv.fr/index.php?option=com_apiportal&view=apitester&usage=api&apitab=tests&apiName=L%C3%A9gifrance&apiId=7e5a0e1d-ffcc-40be-a405-a1a5c1afe950&managerId=3&type=rest&apiVersion=2.4.2&Itemid=179&swaggerVersion=2.0&lang=fr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_HOST = \"https://api.piste.gouv.fr/dila/legifrance/lf-engine-app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les codes ci-dessous sont les requêtes que l'on va envoyer à l'API, ces dernières sont en json et pour en envoyer une il faut taper client.post(api_url, json=code).json(), selon l'endpoint il faudra mettre get à la place de post. \n",
    "\n",
    "Les codes ci-dessous permettent de récupérer un ensemble de documents appartenant à l'ensemble des lois, ordonnance, décrets et arrêtés de Légifrance entre le premier janvier 1996 et le 31 août 2022 qui contiennent au moins un des mots d'une liste définie. Cette liste comprend les termes des taux de délinquances détaillés [ici](database_délinquance.ipynb#Calcul-des-taux-de-délinquance). Ce code est un json et sera envoyé par une méthode post à l'API, elle nous renverra un nombre de résultats limités (100 ici). Ce code est très inspiré de celui disponible sur le [site de l'API](https://piste.gouv.fr/index.php?option=com_apiportal&view=apitester&usage=api&apitab=tests&apiName=L%C3%A9gifrance&apiId=7e5a0e1d-ffcc-40be-a405-a1a5c1afe950&managerId=3&type=rest&apiVersion=2.4.2&Itemid=179&swaggerVersion=2.0&lang=fr) à l'endpoint /search.\n",
    "\n",
    "La seule différence entre les deux codes résident dans le filtre temporel, car comme l'API ne permet pas de renvoyer plus de 10001 résultats par requête alors nous sommes obligés d'en faire deux distinctes et de regrouper les données a posteriori. Cela aurait pu fonctionner sur le fond All en fractionnant encore plus, or nous n'avons pas réussi à faire fonctionner le filtre temporel dans une requête sur All. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_api_LODA_1 = {\n",
    "    \"recherche\": {\n",
    "        \"filtres\": [{\"dates\": {\"start\": \"2008-01-01\", \"end\": \"2022-08-31\"}, \"facette\": \"DATE_SIGNATURE\"}],\n",
    "        \"sort\": \"SIGNATURE_DATE_DESC\",\n",
    "        \"fromAdvancedRecherche\": True,\n",
    "        \"champs\": [\n",
    "            {\n",
    "                \"typeChamp\": \"ALL\",\n",
    "                \"criteres\": [\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": mot, \"operateur\": \"OU\"} for mot in [\n",
    "                        \"délinquance\", \"crime\", \"délit\", \"Homicides\", \"Vols\", \"Stupéfiants\", \"Escroquerie\",\n",
    "                        \"Contrefaçon\", \"Sequestrations\", \"Recels\", \"Proxénétisme\", \"Menaces\", \"Cambriolages\",\n",
    "                        \"infraction\", \"Attentats\", \"dégradations\", \"Outrages\"\n",
    "                    ]\n",
    "                ],\n",
    "                \"operateur\": \"OU\"\n",
    "            }\n",
    "        ],\n",
    "        \"pageSize\": 100,\n",
    "        \"pageNumber\": 1,\n",
    "        \"operateur\": \"ET\",\n",
    "        \"typePagination\": \"DEFAUT\"\n",
    "    },\n",
    "    \"fond\": \"LODA_DATE\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_api_LODA_2 = {\n",
    "    \"recherche\": {\n",
    "        \"filtres\": [{\"dates\": {\"start\": \"1996-01-01\", \"end\": \"2008-12-31\"}, \"facette\": \"DATE_SIGNATURE\"}],\n",
    "        \"sort\": \"SIGNATURE_DATE_DESC\",\n",
    "        \"fromAdvancedRecherche\": True,\n",
    "        \"champs\": [\n",
    "            {\n",
    "                \"typeChamp\": \"ALL\",\n",
    "                \"criteres\": [\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": mot, \"operateur\": \"OU\"} for mot in [\n",
    "                        \"délinquance\", \"crime\", \"délit\", \"Homicides\", \"Vols\", \"Stupéfiants\", \"Escroquerie\",\n",
    "                        \"Contrefaçon\", \"Sequestrations\", \"Recels\", \"Proxénétisme\", \"Menaces\", \"Cambriolages\",\n",
    "                        \"infraction\", \"Attentats\", \"dégradations\", \"Outrages\"\n",
    "                    ]\n",
    "                ],\n",
    "                \"operateur\": \"OU\"\n",
    "            }\n",
    "        ],\n",
    "        \"pageSize\": 100,\n",
    "        \"pageNumber\": 1,\n",
    "        \"operateur\": \"ET\",\n",
    "        \"typePagination\": \"DEFAUT\"\n",
    "    },\n",
    "    \"fond\": \"LODA_DATE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la limite de résultats est de 100 pour une requête nous avons créer deux fonctions complémentaire, l'une parcourant toutes les pages de notre requêtes afin de récupérer tous les résultats tout en s'appuyant sur la seconde qui permet de les sauvegarder dans un fichier json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(results, file_name, current_page):\n",
    "    \"\"\"\n",
    "    Sauvegarde les résultats dans un fichier JSON. Si le fichier existe, ajoute les nouvelles données.\n",
    "\n",
    "    :param results: Liste des résultats à sauvegarder.\n",
    "    :param file_name: Nom du fichier JSON.\n",
    "    :param current_page: La page actuelle traitée.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les données existantes si le fichier existe\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            existing_data = json.load(file)\n",
    "            if not isinstance(existing_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            existing_results = existing_data.get(\"results\", [])\n",
    "            start_page = existing_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        existing_results = []\n",
    "        start_page = 1\n",
    "\n",
    "    # Ajouter les nouveaux résultats\n",
    "    existing_results.extend(results)\n",
    "\n",
    "    # Sauvegarder les résultats mis à jour avec la page actuelle\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"results\": existing_results, \"current_page\": current_page}, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_results(api_host, code, file_name):\n",
    "    ''' \n",
    "    Récupère tous les résultats relatifs à une requête API. \n",
    "\n",
    "    :param api_host: addresse à laquelle envoyer la requête avec le endpoint correspondant \n",
    "    :param code: code permettant de faire la requête correspondante à nos recherches, \n",
    "    pour l'utilisation de la fonction on mettra de fait \n",
    "    collect_all_results(api_host, json = code)\n",
    "    \n",
    "    :return: le nombre total de page \n",
    "    '''\n",
    "\n",
    "    # Charger la page de démarrage depuis le fichier de sauvegarde s'il existe comme il faut \n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            saved_data = json.load(file)\n",
    "            if not isinstance(saved_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            start_page = saved_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        print(f\"Fichier {file_name} introuvable ou mal structuré. Démarrage depuis la première page.\")\n",
    "        start_page = 1\n",
    "\n",
    "    # Initialiser le client OAuth2Session \n",
    "    client = get_client()\n",
    "\n",
    "    # On fixe un temps d'expiration à 55 min car le token dure 60 min\n",
    "    expires_in = 55*60\n",
    "    token_expiry = datetime.now() + timedelta(seconds=expires_in) \n",
    "\n",
    "    # Récupérer le total de résultats et calculer le nombre de pages\n",
    "    response = client.post(api_host, json=code).json()\n",
    "    total_results = response.get(\"totalResultNumber\", 0)\n",
    "    page_size = code[\"recherche\"][\"pageSize\"]\n",
    "    total_pages = math.ceil(total_results / page_size)\n",
    "\n",
    "    print(f\"Total de résultats : {total_results}\")\n",
    "    print(f\"Nombre de pages à récupérer : {total_pages}\")\n",
    "    print(f\"Reprise à partir de la page {start_page}\")\n",
    "\n",
    "    # Liste pour stocker les résultats courants\n",
    "    all_results = []\n",
    "\n",
    "    # On boucle sur le nombre de pages\n",
    "    for page_number in range(start_page, total_pages + 1):\n",
    "\n",
    "        # Vérifier si le token doit être renouvelé\n",
    "        if datetime.now() >= token_expiry:\n",
    "            print(\"Renouvellement du client OAuth...\")\n",
    "            client = get_client()\n",
    "            token_expiry = datetime.now() + timedelta(seconds=expires_in)\n",
    "        \n",
    "        # Récupère le bon numéro de page et lance la requête\n",
    "        print(f\"Récupération de la page {page_number}/{total_pages}...\")\n",
    "        code[\"recherche\"][\"pageNumber\"] = page_number\n",
    "        response = client.post(api_host, json=code).json()\n",
    "        page_results = response.get(\"results\", [])\n",
    "\n",
    "        # Teste si une erreur 503 arrive et arrête la boucle si tel est le cas\n",
    "        if response.get(\"error\") == 503:\n",
    "            print(response)\n",
    "            break\n",
    "\n",
    "        # Affiche une requête toutes les 10 pour du contrôle \n",
    "        if page_number % 10 == 0: \n",
    "            print(response)\n",
    "\n",
    "        # Ajouter les résultats de la page courante\n",
    "        all_results.extend(page_results)\n",
    "\n",
    "        # Sauvegarder les résultats toutes les 20 pages ou à la dernière page\n",
    "        if page_number % 20 == 0 or page_number == total_pages:\n",
    "            print(f\"Ajout des pages jusqu'à la page {page_number} dans {file_name}...\")\n",
    "            save_results_to_file(all_results, file_name, page_number)\n",
    "\n",
    "            # Réinitialiser la liste des résultats sauvegardés\n",
    "            all_results = []\n",
    "\n",
    "    print(f\"Récupération terminée. Dernière page sauvegardée : {total_pages}\")\n",
    "    return total_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1996_to_2008 = collect_all_results(API_HOST+\"/search\", code_api_LODA_2, \"results_1996_to_2008.json\")\n",
    "results_2008_to_2022 = collect_all_results(API_HOST+\"/search\", code_api_LODA_1, \"results_2008_to_2022.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Travail sur les fichiers extraits <a class=\"anchor\" id=\"section13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(source_folder, destination_folder, files_to_move):\n",
    "    \"\"\"\n",
    "    Déplace les fichiers spécifiés d'un dossier source à un dossier destination.\n",
    "\n",
    "    :param source_folder: Chemin du dossier source (str)\n",
    "    :param destination_folder: Chemin du dossier destination (str)\n",
    "    :param files_to_move: Liste des noms de fichiers à déplacer (list)\n",
    "    \"\"\"\n",
    "    # Vérifier si le dossier de destination existe, sinon le créer\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        print(f\"Dossier de destination créé : {destination_folder}\")\n",
    "    \n",
    "    # Parcourir les fichiers à déplacer\n",
    "    for filename in files_to_move:\n",
    "        source_file = os.path.join(source_folder, filename)\n",
    "        destination_file = os.path.join(destination_folder, filename)\n",
    "\n",
    "        # Vérifier si le fichier existe dans le dossier source\n",
    "        if os.path.exists(source_file):\n",
    "            shutil.move(source_file, destination_file)\n",
    "            print(f\"Fichier déplacé : {source_file} -> {destination_file}\")\n",
    "        else:\n",
    "            print(f\"Fichier introuvable : {source_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(\"\",\"data/data_api\", [\"results_1996_to_2008.json\", \"results_2008_to_2022.json\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir récupéré nos deux fichiers json il faut les convertir en dataframe puis en csv afin de les exploiter par la suite. Nous avons ainsi codé une fonction permettant d'extraire la date du titre des documents, une autre qui les convertit en dataframe en ne gardant qu'un nombre limité d'informations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_title(title):\n",
    "    \"\"\"\n",
    "    Extrait la date d'un titre. Si plusieurs dates sont présentes, retourne la plus récente.\n",
    "\n",
    "    :param title: Le titre de l'objet.\n",
    "    :return: La date extraite ou None si aucune date valide n'est trouvée.\n",
    "    \"\"\"\n",
    "    # Regex pour les formats de date\n",
    "    date_patterns = [\n",
    "        r\"(\\d{1,2})\\s+(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)\\s+(\\d{4})\",\n",
    "        r\"(\\d{1,2}/\\d{1,2}/\\d{4})\"\n",
    "    ]\n",
    "\n",
    "    found_dates = []\n",
    "\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, title, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            if len(match) == 3:  # Format \"11 mai 2005\"\n",
    "                day, month, year = match\n",
    "                month_mapping = {\n",
    "                    \"janvier\": 1, \"février\": 2, \"mars\": 3, \"avril\": 4, \"mai\": 5, \"juin\": 6,\n",
    "                    \"juillet\": 7, \"août\": 8, \"septembre\": 9, \"octobre\": 10, \"novembre\": 11, \"décembre\": 12\n",
    "                }\n",
    "                month_num = month_mapping[month.lower()]\n",
    "                found_dates.append(datetime(int(year), month_num, int(day)))\n",
    "            elif len(match) == 1:  # Format \"12/12/2014\"\n",
    "                date_str = match[0]\n",
    "                found_dates.append(datetime.strptime(date_str, \"%d/%m/%Y\"))\n",
    "\n",
    "    if found_dates:\n",
    "        return max(found_dates).strftime(\"%Y-%m-%d\")  # Retourne la date la plus récente au format AAAA-MM-JJ\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_dataframe(json_data):\n",
    "    \"\"\"\n",
    "    Convertit les données JSON en DataFrame en extrayant des champs spécifiques.\n",
    "\n",
    "    :param json_data: Les données JSON à analyser.\n",
    "    :return: Un DataFrame contenant les données extraites.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Accéder à la liste des résultats\n",
    "    results = json_data.get(\"results\", [])\n",
    "\n",
    "    for result in results:\n",
    "        # Extraire les informations requises\n",
    "        titles = result.get(\"titles\")\n",
    "        title_info = titles[0] if isinstance(titles, list) and titles else {}\n",
    "\n",
    "        title = title_info.get(\"title\")\n",
    "        id = title_info.get(\"id\")\n",
    "        date = extract_date_from_title(title)  # Extraire uniquement à partir du titre\n",
    "        nature = result.get(\"nature\")\n",
    "        etat = result.get(\"etat\")\n",
    "        origin = result.get(\"origin\")\n",
    "        date_publication = result.get(\"datePublication\")\n",
    "\n",
    "        # Ajouter les données dans la liste\n",
    "        data.append({\n",
    "            \"Titre\": title,\n",
    "            \"ID\": id,\n",
    "            \"Date\": date,\n",
    "            \"Nature\": nature,\n",
    "            \"Etat\": etat,\n",
    "            \"Origine\": origin,\n",
    "            \"Date Publication\": date_publication\n",
    "        })\n",
    "\n",
    "    # Créer et retourner un DataFrame\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous appliquons ces fonctions à nos deux fichiers json avant de les concaténer sous un unique et de le transformer en csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/data_api/results_1996_to_2008.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "results_1996_to_2008 = results_to_dataframe(json_data)\n",
    "\n",
    "with open(\"data/data_api/results_2008_to_2022.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "results_2008_to_2022 = results_to_dataframe(json_data)\n",
    "\n",
    "results_1996_to_2022 = pd.concat([results_1996_to_2008, results_2008_to_2022], ignore_index=True)\n",
    "\n",
    "results_1996_to_2022.to_csv(\"results_LODA.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par souci de clarté on déplace les documents dans un dossier spécifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(\"\",\"data/data_api\", [\"results_LODA.csv\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des données de Légifrance <a class=\"anchor\" id=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le fichier CSV\n",
    "chemin_fichier = \"data/data_api/results_LODA.csv\"\n",
    "df_loda = pd.read_csv(chemin_fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre d'observations: {len(df_loda)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met la date sous format date de datetime\n",
    "df_loda['Date'] = pd.to_datetime(df_loda['Date']) \n",
    "\n",
    "# On extrait l'année (les 4 premiers caractères) et le mois (caractères à l'index 5 et 6) de la variable Date Publication\n",
    "df_loda['Année'] = df_loda['Date Publication'].str[:4].astype(int)\n",
    "df_loda['Mois'] = df_loda['Date Publication'].str[5:7].astype(int)\n",
    "\n",
    "df_loda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification pertinence des données\n",
    "print(\"Valeurs uniques dans la colonne 'Année' :\")\n",
    "print(df_loda['Année'].unique())\n",
    "\n",
    "print(\"\\nValeurs uniques dans la colonne 'Mois' :\")\n",
    "print(df_loda['Mois'].unique())\n",
    "\n",
    "print(\"\\nValeurs uniques et leur fréquence dans la colonne 'Nature' :\")\n",
    "df_loda['Nature'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se rend compte d'une incohérence : certaines données présentent une date de publication en 2999. Pour pallier ce problème, on se permet d'utiliser la variable Date (souvent différentes de quelques jours à peine, donc peu problématique pour notre analyse réalisée au plus à l'échelle mensuelle) afin de compléter les valeurs des variables 'Mois' et 'Année'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On règle l'incohérence pour les années de publication en 2999 (on utilise la ariable Date)\n",
    "print(f\"Nombre d'incohérences avant traitement (doit être positif): {len(df_loda[df_loda['Année'] == 2999])}\")\n",
    "\n",
    "df_loda.loc[df_loda['Année'] == 2999, 'Année'] = df_loda.loc[df_loda['Année'] == 2999, 'Date'].dt.year\n",
    "df_loda.loc[df_loda['Année'] == 2999, 'Mois'] = df_loda.loc[df_loda['Année'] == 2999, 'Date'].dt.month\n",
    "\n",
    "print(f\"Nombre d'incohérences après traitement (doit être nul): {len(df_loda[df_loda['Année'] == 2999])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se permet également d'exclure, pour le reste de l'étude, les nouvelles normes législatives adoptées de type \"décision\" puisqu'elles ne sont qu'au nombre de 3 sur la période étudiée (trop faible et donc sûrement pas significatif pour le reste de l'analyse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loda = df_loda[df_loda['Nature'] != 'DECISION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'affichage des figures dans le main, nous utilisons souvent la fonction nommée tri_occurrence, définie dans le script Python. Cette fonction prend en entrée un dataframe (ici df_loda) et le transforme en un dataframe qui rend compte des occurrences de publication des textes de loi, en fonction de leur type, mois et année de publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des tableaux de données finalisées <a class=\"anchor\" id=\"section3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information requise pour la connexion au Bucket de Anh Linh sur le MinIO Client cloud\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "MY_BUCKET = \"anhlinh\"\n",
    "fs.ls(MY_BUCKET)\n",
    "\n",
    "# Export du DataFrame dans S3\n",
    "FILE_PATH_OUT_S3_LODA = f\"{MY_BUCKET}/diffusion/df_loda.csv\"\n",
    "with fs.open(FILE_PATH_OUT_S3_LODA, \"w\") as file_out_loda:\n",
    "    df_loda.to_csv(file_out_loda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(f\"{MY_BUCKET}/diffusion\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
