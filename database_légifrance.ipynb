{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de données répertoriant les textes législatifs mentionnant les types d'infractions nous intéressants entre 1996 et 2022 en France\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table des matières\n",
    "\n",
    "* [Récupération des données de Légifrance via une API](#section1)\n",
    "    * [Installation et importation des modules](#section11)\n",
    "    * [Requêtes sur l'API](#section12)\n",
    "    * [Travail sur les fichiers extraits](#section13)\n",
    "* [Nettoyage des données de Légifrance](#section2)\n",
    "* [Sauvegarde des tableaux de données finalisées](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook contient les différents codes qui nous ont permis d'accéder aux données disponibles grâce à l'API Piste de Légifrance.\n",
    "\n",
    "Sur l'API de Légifrance les requêtes doivent être faites sur un fonds, soit un filtre correspondant à une catégorie spécifique de la base de données de Légifrance. Les différents fonds incluent notamment LODA , qui regroupe les lois, les ordonnances, les décrets et les arrêtés, mais il y a aussi CODE pour les documents relatifs aux différents codes et ALL qui permet de faire une requête sur tous les fonds. \n",
    "\n",
    "Au début de notre récupération des données nos requêtes ont été effectuées sur le fond LODA, ensuite CODE et enfin ALL, avant de revenir sur LODA. Cet ordre n'est dû qu'à l'absence de documentation claire, au faible nombre de projets présents sur internet utilisant l'API de Légifrance et d'erreurs que l'on n'arrive pas à expliquer et qui nous empêchent d'avoir accès à l'intégralité des fonds. \n",
    "\n",
    "En fait, lorsqu'on effectue une requête, l'API ne nous renvoie qu'un nombre limité de résultats (100 maximum), donc il faut faire une boucle pour récupérer tous les documents. Les requêtes supérieures au 10001e éléments renvoient une erreur 503, soit une erreur du serveur. Malgré des recherches et des mails envoyés au support et à des personnes travaillant à l'AIFE (Agence pour l'Informatique Financière de l'État) sur l'API de Légifrance nous n'avons pas trouvé de solution à cela. \n",
    "\n",
    "Ainsi, pour l'avancement du projet, nous avons pris la décision de ne récupérer que les données sur le fond LODA, car ce fond est d'après nous le plus pertinent - les modifications des codes étant plus difficiles à étudier sous l'angle quantitatif par rapport aux lois, car il s'agit en partie de modification de vocabulaire employé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données avec le fond LODA <a class=\"anchor\" id=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation et importation des modules <a class=\"anchor\" id=\"section11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (5.3.0)\n",
      "Requirement already satisfied: bs4 in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.0.2)\n",
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (3.10.0)\n",
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (0.14.4)\n",
      "Requirement already satisfied: linearmodels in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (6.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.13.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (2.2.1)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.12/site-packages (from openpyxl->-r requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (2024.12.14)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.12/site-packages (from bs4->-r requirements.txt (line 4)) (4.12.3)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /opt/conda/lib/python3.12/site-packages (from geopandas->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from geopandas->-r requirements.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/conda/lib/python3.12/site-packages (from geopandas->-r requirements.txt (line 5)) (3.7.0)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from geopandas->-r requirements.txt (line 5)) (2.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 6)) (2024.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from requests-oauthlib->-r requirements.txt (line 8)) (3.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 9)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 9)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 9)) (3.2.0)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/conda/lib/python3.12/site-packages (from statsmodels->-r requirements.txt (line 10)) (1.14.1)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/conda/lib/python3.12/site-packages (from statsmodels->-r requirements.txt (line 10)) (1.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4 in /opt/conda/lib/python3.12/site-packages (from linearmodels->-r requirements.txt (line 11)) (1.0.0)\n",
      "Requirement already satisfied: Cython>=3.0.10 in /opt/conda/lib/python3.12/site-packages (from linearmodels->-r requirements.txt (line 11)) (3.0.11)\n",
      "Requirement already satisfied: pyhdfe>=0.1 in /opt/conda/lib/python3.12/site-packages (from linearmodels->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: formulaic>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from linearmodels->-r requirements.txt (line 11)) (1.1.1)\n",
      "Requirement already satisfied: setuptools-scm<9.0.0,>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels->-r requirements.txt (line 11)) (8.1.0)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from formulaic>=1.0.0->linearmodels->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.12/site-packages (from formulaic>=1.0.0->linearmodels->-r requirements.txt (line 11)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /opt/conda/lib/python3.12/site-packages (from formulaic>=1.0.0->linearmodels->-r requirements.txt (line 11)) (1.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from setuptools-scm<9.0.0,>=8.0.0->setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels->-r requirements.txt (line 11)) (75.6.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 4)) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from requests_oauthlib import OAuth2Session\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Thread\n",
    "import zipfile\n",
    "import numpy as np \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Requêtes sur l'API <a class=\"anchor\" id=\"section12\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pour envoyer des requêtes à l'API nous devons d'abord obtenir un token, après s'être inscrit sur le [site de PISTE](https://piste.gouv.fr/). Ce token est obtenu en envoyant une requête contenant mon identifiant client et mon code de client au site d'autorisation, ces derniers seront écrits dans un fichier .env à chaque fois qu'on lance le code pour récupérer un client. Nous avons ensuite une autorisation d'une heure avec ce token pour exploiter l'API de Légifrance. \n",
    "\n",
    "Le code pour récupérer le token est grandement inspiré de celui proposé par le Gitlab de Piste présent à cette [adresse](https://gitlab.com/piste_lab/oauth_connectors/-/blob/master/Python/Oauth2ClientCredentialsTest.py?ref_type=heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client():\n",
    "    \"\"\"\n",
    "    Récupère un client OAuth2Session configuré avec un token d'accès depuis le serveur OAuth. \n",
    "\n",
    "    :return: Un objet OAuth2Session prêt à être utilisé pour des requêtes API.\n",
    "    \"\"\"\n",
    "    TOKEN_URL = \"https://oauth.piste.gouv.fr/api/oauth/token\"\n",
    "\n",
    "    # Charger les identifiants client depuis le fichier .env\n",
    "    load_dotenv()\n",
    "    client_id = os.getenv(\"CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "    # Requête pour obtenir le token\n",
    "    res = requests.post(\n",
    "        TOKEN_URL,\n",
    "        data={\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": client_id,\n",
    "            \"client_secret\": client_secret,\n",
    "            \"scope\": \"openid\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    res.raise_for_status()  # Lever une erreur si la requête échoue\n",
    "\n",
    "    token = res.json()\n",
    "\n",
    "    # Retourner un client OAuth2Session configuré\n",
    "    return OAuth2Session(client_id, token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant en utilisant le lien d'exploitation de l'API auquel on rajoute l'endpoints qui permet d'accéder à ce que l'on veut faire nous récupérons les données. La liste des endpoints pour l'API de Légifrance est disponible [ici](https://piste.gouv.fr/index.php?option=com_apiportal&view=apitester&usage=api&apitab=tests&apiName=L%C3%A9gifrance&apiId=7e5a0e1d-ffcc-40be-a405-a1a5c1afe950&managerId=3&type=rest&apiVersion=2.4.2&Itemid=179&swaggerVersion=2.0&lang=fr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_HOST = \"https://api.piste.gouv.fr/dila/legifrance/lf-engine-app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les codes ci-dessous sont les requêtes que l'on va envoyer à l'API, ces dernières sont en json et pour en envoyer une il faut taper client.post(api_url, json=code).json(), selon l'endpoint il faudra mettre get à la place de post. \n",
    "\n",
    "Les codes ci-dessous permettent de récupérer un ensemble de documents appartenant à l'ensemble des lois, ordonnance, décrets et arrêtés de Légifrance entre le premier janvier 1996 et le 31 août 2022 qui contiennent au moins un des mots d'une liste définie. Cette liste comprend les termes des taux de délinquances détaillés [ici](database_délinquance.ipynb#Calcul-des-taux-de-délinquance). Ce code est un json et sera envoyé par une méthode post à l'API, elle nous renverra un nombre de résultats limités (100 ici). Ce code est très inspiré de celui disponible sur le [site de l'API](https://piste.gouv.fr/index.php?option=com_apiportal&view=apitester&usage=api&apitab=tests&apiName=L%C3%A9gifrance&apiId=7e5a0e1d-ffcc-40be-a405-a1a5c1afe950&managerId=3&type=rest&apiVersion=2.4.2&Itemid=179&swaggerVersion=2.0&lang=fr) à l'endpoint /search.\n",
    "\n",
    "La seule différence entre les deux codes résident dans le filtre temporel, car comme l'API ne permet pas de renvoyer plus de 10001 résultats par requête alors nous sommes obligés d'en faire deux distinctes et de regrouper les données a posteriori. Cela aurait pu fonctionner sur le fond All en fractionnant encore plus, or nous n'avons pas réussi à faire fonctionner le filtre temporel dans une requête sur All. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_api_LODA_1 = {\n",
    "    \"recherche\": {\n",
    "        \"filtres\": [{\"dates\": {\"start\": \"2008-01-01\", \"end\": \"2022-08-31\"}, \"facette\": \"DATE_SIGNATURE\"}],\n",
    "        \"sort\": \"SIGNATURE_DATE_DESC\",\n",
    "        \"fromAdvancedRecherche\": True,\n",
    "        \"champs\": [\n",
    "            {\n",
    "                \"typeChamp\": \"ALL\",\n",
    "                \"criteres\": [\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": mot, \"operateur\": \"OU\"} for mot in [\n",
    "                        \"délinquance\", \"crime\", \"délit\", \"Homicides\", \"Vols\", \"Stupéfiants\", \"Escroquerie\",\n",
    "                        \"Contrefaçon\", \"Sequestrations\", \"Recels\", \"Proxénétisme\", \"Menaces\", \"Cambriolages\",\n",
    "                        \"infraction\", \"Attentats\", \"dégradations\", \"Outrages\"\n",
    "                    ]\n",
    "                ],\n",
    "                \"operateur\": \"OU\"\n",
    "            }\n",
    "        ],\n",
    "        \"pageSize\": 100,\n",
    "        \"pageNumber\": 1,\n",
    "        \"operateur\": \"ET\",\n",
    "        \"typePagination\": \"DEFAUT\"\n",
    "    },\n",
    "    \"fond\": \"LODA_DATE\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_api_LODA_2 = {\n",
    "    \"recherche\": {\n",
    "        \"filtres\": [{\"dates\": {\"start\": \"1996-01-01\", \"end\": \"2008-12-31\"}, \"facette\": \"DATE_SIGNATURE\"}],\n",
    "        \"sort\": \"SIGNATURE_DATE_DESC\",\n",
    "        \"fromAdvancedRecherche\": True,\n",
    "        \"champs\": [\n",
    "            {\n",
    "                \"typeChamp\": \"ALL\",\n",
    "                \"criteres\": [\n",
    "                    {\"typeRecherche\": \"UN_DES_MOTS\", \"valeur\": mot, \"operateur\": \"OU\"} for mot in [\n",
    "                        \"délinquance\", \"crime\", \"délit\", \"Homicides\", \"Vols\", \"Stupéfiants\", \"Escroquerie\",\n",
    "                        \"Contrefaçon\", \"Sequestrations\", \"Recels\", \"Proxénétisme\", \"Menaces\", \"Cambriolages\",\n",
    "                        \"infraction\", \"Attentats\", \"dégradations\", \"Outrages\"\n",
    "                    ]\n",
    "                ],\n",
    "                \"operateur\": \"OU\"\n",
    "            }\n",
    "        ],\n",
    "        \"pageSize\": 100,\n",
    "        \"pageNumber\": 1,\n",
    "        \"operateur\": \"ET\",\n",
    "        \"typePagination\": \"DEFAUT\"\n",
    "    },\n",
    "    \"fond\": \"LODA_DATE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la limite de résultats est de 100 pour une requête nous avons créer deux fonctions complémentaire, l'une parcourant toutes les pages de notre requêtes afin de récupérer tous les résultats tout en s'appuyant sur la seconde qui permet de les sauvegarder dans un fichier json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_file(results, file_name, current_page):\n",
    "    \"\"\"\n",
    "    Sauvegarde les résultats dans un fichier JSON. Si le fichier existe, ajoute les nouvelles données.\n",
    "\n",
    "    :param results: Liste des résultats à sauvegarder.\n",
    "    :param file_name: Nom du fichier JSON.\n",
    "    :param current_page: La page actuelle traitée.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger les données existantes si le fichier existe\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            existing_data = json.load(file)\n",
    "            if not isinstance(existing_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            existing_results = existing_data.get(\"results\", [])\n",
    "            start_page = existing_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        existing_results = []\n",
    "        start_page = 1\n",
    "\n",
    "    # Ajouter les nouveaux résultats\n",
    "    existing_results.extend(results)\n",
    "\n",
    "    # Sauvegarder les résultats mis à jour avec la page actuelle\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"results\": existing_results, \"current_page\": current_page}, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_results(api_host, code, file_name):\n",
    "    ''' \n",
    "    Récupère tous les résultats relatifs à une requête API. \n",
    "\n",
    "    :param api_host: addresse à laquelle envoyer la requête avec le endpoint correspondant \n",
    "    :param code: code permettant de faire la requête correspondante à nos recherches, \n",
    "    pour l'utilisation de la fonction on mettra de fait \n",
    "    collect_all_results(api_host, json = code)\n",
    "    \n",
    "    :return: le nombre total de page \n",
    "    '''\n",
    "\n",
    "    # Charger la page de démarrage depuis le fichier de sauvegarde s'il existe comme il faut \n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            saved_data = json.load(file)\n",
    "            if not isinstance(saved_data, dict):\n",
    "                raise ValueError(\"Le fichier de sauvegarde n'est pas correctement structuré.\")\n",
    "            start_page = saved_data.get(\"current_page\", 1)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        print(f\"Fichier {file_name} introuvable ou mal structuré. Démarrage depuis la première page.\")\n",
    "        start_page = 1\n",
    "\n",
    "    # Initialiser le client OAuth2Session \n",
    "    client = get_client()\n",
    "\n",
    "    # On fixe un temps d'expiration à 55 min car le token dure 60 min\n",
    "    expires_in = 55*60\n",
    "    token_expiry = datetime.now() + timedelta(seconds=expires_in) \n",
    "\n",
    "    # Récupérer le total de résultats et calculer le nombre de pages\n",
    "    response = client.post(api_host, json=code).json()\n",
    "    total_results = response.get(\"totalResultNumber\", 0)\n",
    "    page_size = code[\"recherche\"][\"pageSize\"]\n",
    "    total_pages = math.ceil(total_results / page_size)\n",
    "\n",
    "    print(f\"Total de résultats : {total_results}\")\n",
    "    print(f\"Nombre de pages à récupérer : {total_pages}\")\n",
    "    print(f\"Reprise à partir de la page {start_page}\")\n",
    "\n",
    "    # Liste pour stocker les résultats courants\n",
    "    all_results = []\n",
    "\n",
    "    # On boucle sur le nombre de pages\n",
    "    for page_number in range(start_page, total_pages + 1):\n",
    "\n",
    "        # Vérifier si le token doit être renouvelé\n",
    "        if datetime.now() >= token_expiry:\n",
    "            print(\"Renouvellement du client OAuth...\")\n",
    "            client = get_client()\n",
    "            token_expiry = datetime.now() + timedelta(seconds=expires_in)\n",
    "        \n",
    "        # Récupère le bon numéro de page et lance la requête\n",
    "        print(f\"Récupération de la page {page_number}/{total_pages}...\")\n",
    "        code[\"recherche\"][\"pageNumber\"] = page_number\n",
    "        response = client.post(api_host, json=code).json()\n",
    "        page_results = response.get(\"results\", [])\n",
    "\n",
    "        # Teste si une erreur 503 arrive et arrête la boucle si tel est le cas\n",
    "        if response.get(\"error\") == 503:\n",
    "            print(response)\n",
    "            break\n",
    "\n",
    "        # Affiche une requête toutes les 10 pour du contrôle \n",
    "        if page_number % 10 == 0: \n",
    "            print(response)\n",
    "\n",
    "        # Ajouter les résultats de la page courante\n",
    "        all_results.extend(page_results)\n",
    "\n",
    "        # Sauvegarder les résultats toutes les 20 pages ou à la dernière page\n",
    "        if page_number % 20 == 0 or page_number == total_pages:\n",
    "            print(f\"Ajout des pages jusqu'à la page {page_number} dans {file_name}...\")\n",
    "            save_results_to_file(all_results, file_name, page_number)\n",
    "\n",
    "            # Réinitialiser la liste des résultats sauvegardés\n",
    "            all_results = []\n",
    "\n",
    "    print(f\"Récupération terminée. Dernière page sauvegardée : {total_pages}\")\n",
    "    return total_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier results_1996_to_2008.json introuvable ou mal structuré. Démarrage depuis la première page.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de résultats : 6420\n",
      "Nombre de pages à récupérer : 65\n",
      "Reprise à partir de la page 1\n",
      "Récupération de la page 1/65...\n",
      "Récupération de la page 2/65...\n",
      "Récupération de la page 3/65...\n",
      "Récupération de la page 4/65...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_1996_to_2008 \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_all_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_HOST\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_api_LODA_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults_1996_to_2008.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m results_2008_to_2022 \u001b[38;5;241m=\u001b[39m collect_all_results(API_HOST\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/search\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_api_LODA_1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults_2008_to_2022.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 56\u001b[0m, in \u001b[0;36mcollect_all_results\u001b[0;34m(api_host, code, file_name)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRécupération de la page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_pages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m code[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecherche\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpageNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m page_number\n\u001b[0;32m---> 56\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     57\u001b[0m page_results \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Teste si une erreur 503 arrive et arrête la boucle si tel est le cas\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests_oauthlib/oauth2_session.py:566\u001b[0m, in \u001b[0;36mOAuth2Session.request\u001b[0;34m(self, method, url, data, headers, withhold_token, client_id, client_secret, files, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupplying headers \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and data \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, headers, data)\n\u001b[1;32m    565\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing through key word arguments \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs)\n\u001b[0;32m--> 566\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOAuth2Session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_1996_to_2008 = collect_all_results(API_HOST+\"/search\", code_api_LODA_2, \"results_1996_to_2008.json\")\n",
    "results_2008_to_2022 = collect_all_results(API_HOST+\"/search\", code_api_LODA_1, \"results_2008_to_2022.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Travail sur les fichiers extraits <a class=\"anchor\" id=\"section13\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(source_folder, destination_folder, files_to_move):\n",
    "    \"\"\"\n",
    "    Déplace les fichiers spécifiés d'un dossier source à un dossier destination.\n",
    "\n",
    "    :param source_folder: Chemin du dossier source (str)\n",
    "    :param destination_folder: Chemin du dossier destination (str)\n",
    "    :param files_to_move: Liste des noms de fichiers à déplacer (list)\n",
    "    \"\"\"\n",
    "    # Vérifier si le dossier de destination existe, sinon le créer\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "        print(f\"Dossier de destination créé : {destination_folder}\")\n",
    "    \n",
    "    # Parcourir les fichiers à déplacer\n",
    "    for filename in files_to_move:\n",
    "        source_file = os.path.join(source_folder, filename)\n",
    "        destination_file = os.path.join(destination_folder, filename)\n",
    "\n",
    "        # Vérifier si le fichier existe dans le dossier source\n",
    "        if os.path.exists(source_file):\n",
    "            shutil.move(source_file, destination_file)\n",
    "            print(f\"Fichier déplacé : {source_file} -> {destination_file}\")\n",
    "        else:\n",
    "            print(f\"Fichier introuvable : {source_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(\"\",\"data/data_api\", [\"results_1996_to_2008.json\", \"results_2008_to_2022.json\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir récupéré nos deux fichiers json il faut les convertir en dataframe puis en csv afin de les exploiter par la suite. Nous avons ainsi codé une fonction permettant d'extraire la date du titre des documents, une autre qui les convertit en dataframe en ne gardant qu'un nombre limité d'informations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_title(title):\n",
    "    \"\"\"\n",
    "    Extrait la date d'un titre. Si plusieurs dates sont présentes, retourne la plus récente.\n",
    "\n",
    "    :param title: Le titre de l'objet.\n",
    "    :return: La date extraite ou None si aucune date valide n'est trouvée.\n",
    "    \"\"\"\n",
    "    # Regex pour les formats de date\n",
    "    date_patterns = [\n",
    "        r\"(\\d{1,2})\\s+(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)\\s+(\\d{4})\",\n",
    "        r\"(\\d{1,2}/\\d{1,2}/\\d{4})\"\n",
    "    ]\n",
    "\n",
    "    found_dates = []\n",
    "\n",
    "    for pattern in date_patterns:\n",
    "        matches = re.findall(pattern, title, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            if len(match) == 3:  # Format \"11 mai 2005\"\n",
    "                day, month, year = match\n",
    "                month_mapping = {\n",
    "                    \"janvier\": 1, \"février\": 2, \"mars\": 3, \"avril\": 4, \"mai\": 5, \"juin\": 6,\n",
    "                    \"juillet\": 7, \"août\": 8, \"septembre\": 9, \"octobre\": 10, \"novembre\": 11, \"décembre\": 12\n",
    "                }\n",
    "                month_num = month_mapping[month.lower()]\n",
    "                found_dates.append(datetime(int(year), month_num, int(day)))\n",
    "            elif len(match) == 1:  # Format \"12/12/2014\"\n",
    "                date_str = match[0]\n",
    "                found_dates.append(datetime.strptime(date_str, \"%d/%m/%Y\"))\n",
    "\n",
    "    if found_dates:\n",
    "        return max(found_dates).strftime(\"%Y-%m-%d\")  # Retourne la date la plus récente au format AAAA-MM-JJ\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_dataframe(json_data):\n",
    "    \"\"\"\n",
    "    Convertit les données JSON en DataFrame en extrayant des champs spécifiques.\n",
    "\n",
    "    :param json_data: Les données JSON à analyser.\n",
    "    :return: Un DataFrame contenant les données extraites.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Accéder à la liste des résultats\n",
    "    results = json_data.get(\"results\", [])\n",
    "\n",
    "    for result in results:\n",
    "        # Extraire les informations requises\n",
    "        titles = result.get(\"titles\")\n",
    "        title_info = titles[0] if isinstance(titles, list) and titles else {}\n",
    "\n",
    "        title = title_info.get(\"title\")\n",
    "        id = title_info.get(\"id\")\n",
    "        date = extract_date_from_title(title)  # Extraire uniquement à partir du titre\n",
    "        nature = result.get(\"nature\")\n",
    "        etat = result.get(\"etat\")\n",
    "        origin = result.get(\"origin\")\n",
    "        date_publication = result.get(\"datePublication\")\n",
    "\n",
    "        # Ajouter les données dans la liste\n",
    "        data.append({\n",
    "            \"Titre\": title,\n",
    "            \"ID\": id,\n",
    "            \"Date\": date,\n",
    "            \"Nature\": nature,\n",
    "            \"Etat\": etat,\n",
    "            \"Origine\": origin,\n",
    "            \"Date Publication\": date_publication\n",
    "        })\n",
    "\n",
    "    # Créer et retourner un DataFrame\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous appliquons ces fonctions à nos deux fichiers json avant de les concaténer sous un unique et de le transformer en csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/data_api/results_1996_to_2008.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "results_1996_to_2008 = results_to_dataframe(json_data)\n",
    "\n",
    "with open(\"data/data_api/results_2008_to_2022.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "results_2008_to_2022 = results_to_dataframe(json_data)\n",
    "\n",
    "results_1996_to_2022 = pd.concat([results_1996_to_2008, results_2008_to_2022], ignore_index=True)\n",
    "\n",
    "results_1996_to_2022.to_csv(\"results_LODA.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par souci de clarté on déplace les documents dans un dossier spécifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(\"\",\"data/data_api\", [\"results_LODA.csv\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des données de Légifrance <a class=\"anchor\" id=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le fichier CSV\n",
    "chemin_fichier = \"data/data_api/results_LODA.csv\"\n",
    "df_loda = pd.read_csv(chemin_fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre d'observations: {len(df_loda)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met la date sous format date de datetime\n",
    "df_loda['Date'] = pd.to_datetime(df_loda['Date']) \n",
    "\n",
    "# On extrait l'année (les 4 premiers caractères) et le mois (caractères à l'index 5 et 6) de la variable Date Publication\n",
    "df_loda['Année'] = df_loda['Date Publication'].str[:4].astype(int)\n",
    "df_loda['Mois'] = df_loda['Date Publication'].str[5:7].astype(int)\n",
    "\n",
    "df_loda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification pertinence des données\n",
    "print(\"Valeurs uniques dans la colonne 'Année' :\")\n",
    "print(df_loda['Année'].unique())\n",
    "\n",
    "print(\"\\nValeurs uniques dans la colonne 'Mois' :\")\n",
    "print(df_loda['Mois'].unique())\n",
    "\n",
    "print(\"\\nValeurs uniques et leur fréquence dans la colonne 'Nature' :\")\n",
    "df_loda['Nature'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se rend compte d'une incohérence : certaines données présentent une date de publication en 2999. Pour pallier ce problème, on se permet d'utiliser la variable Date (souvent différentes de quelques jours à peine, donc peu problématique pour notre analyse réalisée au plus à l'échelle mensuelle) afin de compléter les valeurs des variables 'Mois' et 'Année'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On règle l'incohérence pour les années de publication en 2999 (on utilise la ariable Date)\n",
    "print(f\"Nombre d'incohérences avant traitement (doit être positif): {len(df_loda[df_loda['Année'] == 2999])}\")\n",
    "\n",
    "df_loda.loc[df_loda['Année'] == 2999, 'Année'] = df_loda.loc[df_loda['Année'] == 2999, 'Date'].dt.year\n",
    "df_loda.loc[df_loda['Année'] == 2999, 'Mois'] = df_loda.loc[df_loda['Année'] == 2999, 'Date'].dt.month\n",
    "\n",
    "print(f\"Nombre d'incohérences après traitement (doit être nul): {len(df_loda[df_loda['Année'] == 2999])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se permet également d'exclure, pour le reste de l'étude, les nouvelles normes législatives adoptées de type \"décision\" puisqu'elles ne sont qu'au nombre de 3 sur la période étudiée (trop faible et donc sûrement pas significatif pour le reste de l'analyse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loda = df_loda[df_loda['Nature'] != 'DECISION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'affichage des figures dans le main, nous utilisons souvent la fonction nommée tri_occurrence, définie dans le script Python. Cette fonction prend en entrée un dataframe (ici df_loda) et le transforme en un dataframe qui rend compte des occurrences de publication des textes de loi, en fonction de leur type, mois et année de publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des tableaux de données finalisées <a class=\"anchor\" id=\"section3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information requise pour la connexion au Bucket de Anh Linh sur le MinIO Client cloud\n",
    "fs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n",
    "MY_BUCKET = \"anhlinh\"\n",
    "fs.ls(MY_BUCKET)\n",
    "\n",
    "# Export du DataFrame dans S3\n",
    "FILE_PATH_OUT_S3_LODA = f\"{MY_BUCKET}/diffusion/df_loda.csv\"\n",
    "with fs.open(FILE_PATH_OUT_S3_LODA, \"w\") as file_out_loda:\n",
    "    df_loda.to_csv(file_out_loda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(f\"{MY_BUCKET}/diffusion\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
